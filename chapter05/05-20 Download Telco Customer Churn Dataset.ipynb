{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "c512e315-70e7-424e-92cd-9e09e2ad01cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src= \"https://cdn.oreillystatic.com/images/sitewide-headers/oreilly_logo_mark_red.svg\"/>&nbsp;&nbsp;<font size=\"16\"><b>AI, ML and GenAI in the Lakehouse<b></font></span>\n",
    "<img style=\"float: left; margin: 0px 15px 15px 0px; width:30%; height: auto;\" src=\"https://i.imgur.com/pQvJTVf.jpeg\"   />   \n",
    "\n",
    "\n",
    " \n",
    "  \n",
    "   Name:          chapter 05-7-Load Telco Customer Churn Dataset\n",
    " \n",
    "   Author:    Bennie Haelen\n",
    "   Date:      12-24-2024\n",
    "\n",
    "   Purpose:   This notebook will read the customer transaction analysis dataset from Kaggle and transform the data into features\n",
    "                 \n",
    "      An outline of the different sections in this notebook:\n",
    "        1 - Read the Delta table witeh the housing prices\n",
    "        2 - Start the modeling phase\n",
    "            2-1 - Perform a train/test split of the data\n",
    "            2-2 - Investigate the Shape of the datasets\n",
    "            2-3 - Convert our training Pandas Dataframe to Spark\n",
    "            2-4 - Start the AutoML Regression\n",
    "        3 - Study the results of the regression and make predictions\n",
    "            3-1 - Retrieve the URI of the best model\n",
    "            3-2 - Create the Test Features\n",
    "            3-3 - Load the best model from the MLflow function\n",
    "            3-4 - Use the model to make prediction\n",
    "            3-5 - Combine predictions and actual\n",
    "            3-6 - Create a plot comparing the actuals with the predictions\n",
    "            3-7 - Create a joint plot of actual vs predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9828c36-5416-4543-9e8a-242ea78a317a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "#Handle Pre-Requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23e5e81d-ff83-45dd-8ba4-cbcbd4b35214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Make sure that kaggle and kagglehub are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cdeeb79-e931-443e-969b-bf668178d807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2ed4b88-37d1-469f-939d-f565a0471b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "278b49bd-c640-44ae-94b3-ac1ed0066016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Make sure to run the notebook with our constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48e1ee3d-eabc-4a53-9a39-dfc8c2b438ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from databricks import feature_store\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3746b6b2-a8d0-4c1e-bc20-a52f76ba8af5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../common/Constants\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "970f2c11-9f4f-4b3f-97df-ae0508cb7742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File locations\n",
    "TELCO_LOCAL_FILE_NAME = \"Telco_customer_churn.csv\"\n",
    "KAGGLE_FILE_LOCATION    = \"aadityabansalcodes/telecommunications-industry-customer-churn-dataset/versions/4\"\n",
    "\n",
    "# Table Name\n",
    "FEATURE_TABLE_NAME   = \"customer_churn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f1bd61f-4ecb-4eb7-a222-fd40088a0336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Use KaggleHub to download the Kaggle Dataset\n",
    "[Link to the dataset] https://www.kaggle.com/datasets/aadityabansalcodes/telecommunications-industry-customer-churn-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44fc0ee6-e079-4617-9800-a81c3520c73c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Download the dataset to a local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d069f78-e908-49be-b399-65586d0c5aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the 'kagglehub' module to interact with Kaggle datasets.\n",
    "import kagglehub  \n",
    "\n",
    "# Download the latest version of the specified dataset.\n",
    "# 'dataset_download' takes the dataset identifier as an argument.\n",
    "# In this case, it downloads the dataset 'telecommunications-industry-customer-churn-dataset' by the user 'aadityabansalcodes',\n",
    "# Which is the Telco Customer Churn Dataset\n",
    "local_path = kagglehub.dataset_download(KAGGLE_FILE_LOCATION)\n",
    "\n",
    "# Print the local file path where the dataset files have been downloaded.\n",
    "print(\"Path to dataset files:\", local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c567b85e-bfc0-4ee0-b9b7-9194f6c66223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "##Copy the local file to our DBFS datasets location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5b9fae-04c8-4f83-989d-1af1e171a2ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Construct the full local path by appending the file name to the existing local directory path\n",
    "local_path = f\"{local_path}/{TELCO_LOCAL_FILE_NAME}\"\n",
    "\n",
    "# Print the local path to verify correctness\n",
    "print(f\"The file has been downloaded to local path: {local_path}\")  \n",
    "\n",
    "# Define the DBFS path where you want to move the file\n",
    "# This path specifies where the file will be stored in the Databricks File System (DBFS)\n",
    "dbfs_path = f\"{DBFS_DATASET_DIRECTORY}/{TELCO_LOCAL_FILE_NAME}\"\n",
    "print(f\"The file will be copied to the dfbs location: {dbfs_path}\")\n",
    "\n",
    "# Use shutil.copy() to move the file from the local path to the DBFS path\n",
    "# This function copies the file to the specified DBFS directory, making it accessible to Databricks\n",
    "shutil.copy(local_path, dbfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0d51217-7210-4985-b2e0-411093cd2458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls dbfs:/FileStore/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0aaafd-ace3-4c70-96d2-a6f74a677342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8f81bb5-ddca-45e8-9176-72d97568c1ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if the path starts with '/dbfs'\n",
    "# The '/dbfs' prefix is used for local file system access, but Spark needs the path in DBFS format\n",
    "if dbfs_path.startswith(\"/dbfs\"):\n",
    "    dbfs_path = dbfs_path[5:]  # Remove the first 5 characters to strip the '/dbfs' prefix\n",
    "\n",
    "# Print the adjusted DBFS path to verify it has been modified correctly\n",
    "print(f\"Adjusted DBFS path: {dbfs_path}\")\n",
    "\n",
    "# Read the CSV file from the adjusted DBFS path using Spark\n",
    "# The 'header=True' option specifies that the first row of the file contains column names\n",
    "df = spark.read.csv(dbfs_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display the first 5 rows of the DataFrame to verify successful loading\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15c63b34-68a7-4ce2-b4e1-19957a684099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean column names (remove spaces)\n",
    "for col_name in df.columns:\n",
    "    df = df.withColumnRenamed(col_name, col_name.replace(\" \", \"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0e00562-9526-4f01-91fa-72104b5a84aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS book_ai_ml_lakehouse.feature_store_db;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ce1a5e-744f-4092-be6d-a1f22ad5b1ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the fully qualified feature table name (Unity Catalog)\n",
    "feature_table_name = f\"{CATALOG_NAME}.{FEATURE_STORE_DB}.{FEATURE_TABLE_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da5a113-dbde-44b5-9e57-0e7d2346a376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Feature Store client\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "# Select initial features\n",
    "\n",
    "initial_features = df.select(\n",
    "    \"CustomerID\",\n",
    "    \"Gender\",\n",
    "    \"SeniorCitizen\",\n",
    "    \"Partner\",\n",
    "    \"Dependents\",\n",
    "    \"Tenure_Months\",\n",
    "    \"Monthly_Charges\",\n",
    "    \"Churn_Value\"\n",
    ")\n",
    "\n",
    "# Write initial features to the Feature Store\n",
    "fs.create_table(\n",
    "    name=\"telco_churn_demographics\",\n",
    "    primary_keys=\"CustomerID\",\n",
    "    df=initial_features,\n",
    "    description=\"Initial feature set for Telco Customer Churn demographics\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2897bf19-3817-4d61-8e61-fa8f985e0973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, lit, mean\n",
    "\n",
    "# Data Cleaning and Preprocessing\n",
    "# Replace missing or null values in the dataset\n",
    "df = df.fillna({\"Total Charges\": 0.0})\n",
    "\n",
    "# Cast 'SeniorCitizen' to a string for encoding\n",
    "df = df.withColumn(\"SeniorCitizen\", when(col(\"Senior Citizen\") == 1, \"Yes\").otherwise(\"No\"))\n",
    "\n",
    "# Create new features for demonstration\n",
    "df = df.withColumn(\"TotalChargesPerMonth\", col(\"Total Charges\") / (col(\"Tenure Months\") + lit(1)))\n",
    "df = df.withColumn(\"IsLongTermContract\", when(col(\"Contract\") == \"Two year\", lit(1)).otherwise(lit(0)))\n",
    "\n",
    "# Create a DataFrame for features\n",
    "feature_df = df.select(\n",
    "    \"customerID\",\n",
    "    \"Tenure Months\",\n",
    "    \"Monthly Charges\",\n",
    "    \"TotalChargesPerMonth\",\n",
    "    \"IsLongTermContract\",\n",
    "    \"SeniorCitizen\",\n",
    "    \"CLTV\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6080560715861818,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "05-20 Download Telco Customer Churn Dataset",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
