{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d808c93-1cb7-4bc3-be92-5148b27a4ca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<img src= \"https://cdn.oreillystatic.com/images/sitewide-headers/oreilly_logo_mark_red.svg\"/>&nbsp;&nbsp;<font size=\"16\"><b>AI, ML and GenAI in the Lakehouse<b></font></span>\n",
    "<img style=\"float: left; margin: 0px 15px 15px 0px; width:30%; height: auto;\" src=\"https://i.imgur.com/pQvJTVf.jpeg\"   />   \n",
    "\n",
    "\n",
    " \n",
    "  \n",
    "   Name:          09-01-Machine Learning at Scale with the NYC Taxi Dataset\n",
    " \n",
    "   Author:    Bennie Haelen\n",
    "   Date:      7-5-2025\n",
    "\n",
    "   Purpose:   This notebook demonstrates how to prepare data for a RAG solution\n",
    "                 \n",
    "      An outline of the different sections in this notebook:\n",
    "        1 - Data Ingestion and Initial Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab4f158-1a7d-4709-bbfb-ca2075a4787a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " %pip install -qq -U llama-index pydantic wikipedia-api requests beautifulsoup4\n",
    " %pip install transformers[torch]\n",
    " \n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1055d3d-0f30-4028-a885-2459b548f22f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./9-Common-Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4582a5e-6a10-40d4-b099-46d0ff377029",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Task 1: Fetch Wikipedia articles and load them into a DataFrame\n",
    "\"\"\"\n",
    "To start, you need to fetch Wikipedia articles and load them into a DataFrame.\n",
    "\n",
    "Steps:\n",
    "1. Define a list of Wikipedia article titles to fetch\n",
    "2. Create a function to fetch Wikipedia content\n",
    "3. Use Spark to create a DataFrame with the articles\n",
    "4. Ensure that each article is represented as a separate record in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec7bf85-9a6c-492a-b7a7-ca7c25d11a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Import required libraries\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.utils import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Iterator\n",
    "from pyspark.sql.functions import col, udf, length, pandas_udf, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "import os\n",
    "import pandas as pd \n",
    "import io\n",
    "import requests\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3369e89e-47a4-42d5-a939-eb583cd1a5d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure Spark for processing\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10)\n",
    "table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.lab_wikipedia_raw_text\"\n",
    "\n",
    "## Define Wikipedia articles to fetch (you can modify this list)\n",
    "WIKIPEDIA_TOPICS = [\n",
    "    \"Artificial_intelligence\",\n",
    "    \"Machine_learning\", \n",
    "    \"Deep_learning\",\n",
    "    \"Natural_language_processing\",\n",
    "    \"Computer_vision\",\n",
    "    \"Reinforcement_learning\",\n",
    "    \"Neural_network\",\n",
    "    \"Large_language_model\",\n",
    "    \"Transformer_(machine_learning_model)\",\n",
    "    \"Generative_artificial_intelligence\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "824ff73b-c8e6-4960-baae-e9461fec362b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetch_wikipedia_article(title):\n",
    "    \"\"\"\n",
    "    Fetch Wikipedia article content using the Wikipedia API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use Wikipedia API to get article content\n",
    "        url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{title}\"\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            summary_data = response.json()\n",
    "            \n",
    "            # Get full article content\n",
    "            content_url = f\"https://en.wikipedia.org/w/api.php\"\n",
    "            params = {\n",
    "                'action': 'query',\n",
    "                'format': 'json',\n",
    "                'titles': title.replace('_', ' '),\n",
    "                'prop': 'extracts',\n",
    "                'exintro': False,\n",
    "                'explaintext': True,\n",
    "                'exsectionformat': 'plain'\n",
    "            }\n",
    "            \n",
    "            content_response = requests.get(content_url, params=params)\n",
    "            if content_response.status_code == 200:\n",
    "                content_data = content_response.json()\n",
    "                pages = content_data['query']['pages']\n",
    "                page_id = list(pages.keys())[0]\n",
    "                \n",
    "                if page_id != '-1':  # Article exists\n",
    "                    content = pages[page_id].get('extract', '')\n",
    "                    return {\n",
    "                        'title': title,\n",
    "                        'content': content,\n",
    "                        'url': f\"https://en.wikipedia.org/wiki/{title}\",\n",
    "                        'fetch_time': datetime.now()\n",
    "                    }\n",
    "        \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {title}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fd1f851-b757-414b-bbdd-983148bcd97d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Fetch Wikipedia articles\n",
    "print(\"Fetching Wikipedia articles...\")\n",
    "articles_data = []\n",
    "\n",
    "for topic in WIKIPEDIA_TOPICS:\n",
    "    print(f\"Fetching: {topic}\")\n",
    "    article_data = fetch_wikipedia_article(topic)\n",
    "    if article_data and article_data['content']:\n",
    "        articles_data.append(article_data)\n",
    "        print(f\"✓ Successfully fetched {topic} ({len(article_data['content'])} characters)\")\n",
    "    else:\n",
    "        print(f\"✗ Failed to fetch {topic}\")\n",
    "\n",
    "print(f\"\\nSuccessfully fetched {len(articles_data)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8da0db87-3286-459f-99a2-c124e0046836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Create DataFrame from fetched articles\n",
    "schema = StructType([\n",
    "    StructField(\"article_title\", StringType(), True),\n",
    "    StructField(\"content\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"fetch_time\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7972dab5-91ad-4406-95de-b20e15de7c27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame first, then to Spark DataFrame\n",
    "pandas_df = pd.DataFrame(articles_data)\n",
    "pandas_df.columns = ['article_title', 'content', 'url', 'fetch_time']\n",
    "\n",
    "df = spark.createDataFrame(pandas_df, schema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77ceadfd-12db-4509-a8ce-cf155343b60d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Task 2: Extract and clean the text content and split it into manageable chunks\n",
    "\"\"\"\n",
    "Next, clean and split the text content into manageable chunks.\n",
    "\n",
    "Steps:\n",
    "1. Define a function to clean Wikipedia text\n",
    "2. Define a function to split the text content into chunks\n",
    "3. Apply the functions to create a new DataFrame with the text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c7ec14c-4c0d-4949-9a99-cb35e4c4b498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Define a function to clean Wikipedia text\n",
    "def clean_wikipedia_text(text):\n",
    "    \"\"\"\n",
    "    Clean Wikipedia text by removing special formatting and references\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove references like [1], [2], etc.\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    \n",
    "    # Remove multiple whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special Wikipedia formatting\n",
    "    text = re.sub(r'={2,}.*?={2,}', '', text)  # Remove section headers\n",
    "    \n",
    "    # Clean up extra spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31f14725-0b29-4151-a9f5-ebf34673eb6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Define a function to split the text content into chunks\n",
    "@pandas_udf(\"array<string>\")\n",
    "def read_as_chunk(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    # Set llama2 as tokenizer\n",
    "    set_global_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "    )\n",
    "    # Sentence splitter from llama_index to split on sentences\n",
    "    splitter = SentenceSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    \n",
    "    def process_batch(content_series):\n",
    "        result = []\n",
    "        for content in content_series:\n",
    "            if content:\n",
    "                # Clean the Wikipedia text\n",
    "                cleaned_content = clean_wikipedia_text(content)\n",
    "                \n",
    "                # Create Document object for llama_index\n",
    "                doc = Document(text=cleaned_content)\n",
    "                \n",
    "                # Split into chunks\n",
    "                chunks = splitter.split_text(cleaned_content)\n",
    "                \n",
    "                # Filter out very short chunks\n",
    "                valid_chunks = [chunk for chunk in chunks if len(chunk.strip()) > 100]\n",
    "                result.append(valid_chunks)\n",
    "            else:\n",
    "                result.append([])\n",
    "        \n",
    "        return pd.Series(result)\n",
    "    \n",
    "    for batch in batch_iter:\n",
    "        yield process_batch(batch)\n",
    "\n",
    "## Apply the chunking function\n",
    "df_chunks = (\n",
    "    df.withColumn(\"chunks\", read_as_chunk(col(\"content\")))\n",
    "    .select(\"article_title\", \"url\", explode(\"chunks\").alias(\"content\"))\n",
    "    .filter(length(\"content\") > 100)  # Filter out very short chunks\n",
    ")\n",
    "\n",
    "df_chunks.display()\n",
    "print(f\"Created {df_chunks.count()} text chunks from Wikipedia articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "452e055a-8d8a-4ef5-a601-b47fb69bc6df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "def get_embedding_udf(batch_size=10):\n",
    "    import mlflow.deployments\n",
    "    deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "    \n",
    "    def embed(text):\n",
    "        try:\n",
    "            if not text or not str(text).strip():\n",
    "                return [0.0] * 1024\n",
    "            \n",
    "            response = deploy_client.predict(\n",
    "                endpoint=\"databricks-bge-large-en\",\n",
    "                inputs={\"input\": [str(text)]}\n",
    "            )\n",
    "            return response.data[0]['embedding']\n",
    "        except Exception as e:\n",
    "            print(f\"Error embedding text: {e}\")\n",
    "            return [0.0] * 1024\n",
    "    \n",
    "    return udf(embed, ArrayType(FloatType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee8327c-7dc2-41dd-b84e-061cd96b97c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Computing embeddings for text chunks...\")\n",
    "df_chunk_emd = df_chunks.withColumn(\"embedding\", get_embedding_udf()(col(\"content\")))\n",
    "df_chunk_emd.display()\n",
    "print(f\"Computed embeddings for {df_chunk_emd.count()} text chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26c5253d-a817-4009-aa6e-7c9cc6cdbf93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Finally, create a Delta table to store the computed embeddings.\n",
    "\n",
    "Steps:\n",
    "1. Create the Delta table schema\n",
    "2. Save the DataFrame containing the computed embeddings as a Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f2d030-571c-45b7-a78c-b8f2e9912a8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the Delta table\n",
    "embedding_table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.lab_wikipedia_text_embeddings\"\n",
    "\n",
    "# SQL command to create the table\n",
    "create_table_sql = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {embedding_table_name} (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  article_title STRING,\n",
    "  url STRING,\n",
    "  content STRING,\n",
    "  embedding ARRAY<FLOAT>\n",
    ") TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_table_sql)\n",
    "print(f\"Created Delta table: {embedding_table_name}\")\n",
    "\n",
    "## Save the DataFrame as a Delta table\n",
    "df_chunk_emd.write.mode(\"append\").saveAsTable(embedding_table_name)\n",
    "print(f\"Saved {df_chunk_emd.count()} records to Delta table\")\n",
    "\n",
    "## Verify the data was saved correctly\n",
    "verification_df = spark.sql(f\"SELECT COUNT(*) as total_records FROM {embedding_table_name}\")\n",
    "verification_df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83dcb87e-9247-49aa-ac75-9b1c7b124f2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM book_ai_ml_lakehouse.rag.lab_wikipedia_text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f1dad60-e276-43e1-b088-fe6d8c6674b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4963720176795107,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "9-2 RAG Data Preparation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
