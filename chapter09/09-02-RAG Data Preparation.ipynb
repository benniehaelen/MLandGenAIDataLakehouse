{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5420375908487858,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "1fd97d06-efa9-4fb2-8a94-6e95095a8918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display:flex; align-items:flex-start; margin-bottom:1rem;\">\n",
    "  <!-- Left: Book cover -->\n",
    "  <img\n",
    "    src=\"https://adb-1376134742576436.16.azuredatabricks.net/files/Images/book_cover.JPG\"\n",
    "    style=\"width:35%; margin-right:1rem; border-radius:4px; box-shadow:0 2px 6px rgba(0,0,0,0.1);\"\n",
    "    alt=\"Book Cover\"/>\n",
    "  <!-- Right: Metadata -->\n",
    "  <div style=\"flex:1;\">\n",
    "    <!-- O'Reilly logo above title -->\n",
    "    <div style=\"display:flex; flex-direction:column; align-items:flex-start; margin-bottom:0.75rem;\">\n",
    "      <img\n",
    "        src=\"https://cdn.oreillystatic.com/images/sitewide-headers/oreilly_logo_mark_red.svg\"\n",
    "        style=\"height:2rem; margin-bottom:0.25rem;\"\n",
    "        alt=\"O‘Reilly\"/>\n",
    "      <span style=\"font-size:1.75rem; font-weight:bold; line-height:1.2;\">\n",
    "        AI, ML and GenAI in the Lakehouse\n",
    "      </span>\n",
    "    </div>\n",
    "    <!-- Details, now each on its own line -->\n",
    "    <div style=\"font-size:0.9rem; color:#555; margin-bottom:1rem; line-height:1.4;\">\n",
    "      <div><strong>Name:</strong> 09-02-RAG Data Preparation</div>\n",
    "      <div><strong>Author:</strong> Bennie Haelen</div>\n",
    "      <div><strong>Date:</strong> 7-5-2025</div>\n",
    "    </div>\n",
    "    <!-- Purpose -->\n",
    "    <div style=\"font-weight:600; margin-bottom:0.75rem;\">\n",
    "      Purpose: This notebook demonstrates how to prepare data for a RAG solution\n",
    "    </div>\n",
    "    <!-- Outline -->\n",
    "    <div style=\"margin-top:0;\">\n",
    "      <h3 style=\"margin:0 0 0.25rem;\">Table of Contents</h3>\n",
    "      <ol style=\"padding-left:1.25rem; margin:0; color:#333;\">\n",
    "        <li>Fetch Wikipedia articles and load them into a DataFrame</li>\n",
    "        <li>Extract/clean the text content-split it into manageable chunks</li>\n",
    "        <li>Calculate the embeddings</li>\n",
    "        <li>Store the embeddings in a Delta file</li>\n",
    "      </ol>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5420375908487858,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a77d486-49ef-434c-8e51-5b4595341286",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Notebook Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5420375908487858,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b580edc-d678-47fa-896f-a4d123186854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Load our Common Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab4f158-1a7d-4709-bbfb-ca2075a4787a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " %pip install -qq -U llama-index pydantic wikipedia-api requests beautifulsoup4\n",
    " %pip install transformers[torch]\n",
    " \n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5420375908487858,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3b0c52f-b0c5-43d2-90aa-a7bf3d38ef1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Load our Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1055d3d-0f30-4028-a885-2459b548f22f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./9-00-Common-Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5420375908487858,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4582a5e-6a10-40d4-b099-46d0ff377029",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fetch Wikipedia articles and load them into a DataFrame\n",
    "\n",
    "To start, you need to fetch Wikipedia articles and load them into a DataFrame.\n",
    "\n",
    "Steps:\n",
    "1. Define a list of Wikipedia article titles to fetch\n",
    "2. Create a function to fetch Wikipedia content\n",
    "3. Use Spark to create a DataFrame with the articles\n",
    "4. Ensure that each article is represented as a separate record in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec7bf85-9a6c-492a-b7a7-ca7c25d11a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Import required libraries\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.utils import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Iterator\n",
    "from pyspark.sql.functions import col, udf, length, pandas_udf, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType\n",
    "import os\n",
    "import pandas as pd \n",
    "import io\n",
    "import requests\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5420375908487858,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b820984-e9a1-40a4-b2db-7edc8154b007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingest the Wikipedia articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5420375908487858,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ebccdce-b0f2-4b31-84c3-b23774011a97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Initialize the Wikipedia topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3369e89e-47a4-42d5-a939-eb583cd1a5d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#This determines the maximum number of records per Arrow \n",
    "# batch during data conversion between Spark and Python.\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10)\n",
    "\n",
    "# Set our table name\n",
    "table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.lab_wikipedia_raw_text\"\n",
    "\n",
    "# Define Wikipedia articles to fetch \n",
    "WIKIPEDIA_TOPICS = [\n",
    "    \"Neural_network_(machine_learning)\",\n",
    "    \"Supervised_learning\",\n",
    "    \"Natural_language_processing\",\n",
    "    \"Symbolic_artificial_intelligence\",\n",
    "    \"Machine_learning\",\n",
    "    \"Deep_learning\",\n",
    "    \"Neural_network\",\n",
    "    \"Neural_network_(machine_learning)\",\n",
    "    \"Decision_tree_learning\",\n",
    "    \"Support_vector_machine\",\n",
    "    \"Random_forest\",\n",
    "    \"Gradient_boosting\",\n",
    "    \"K-means_clustering\",\n",
    "    \"Principal_component_analysis\",\n",
    "    \"Linear_regression\",\n",
    "    \"Logistic_regression\",\n",
    "    \"Naive_Bayes_classifier\",\n",
    "    \"K-nearest_neighbors_algorithm\",\n",
    "    \"Ensemble_learning\",\n",
    "    \"Cross-validation_(statistics)\",\n",
    "    \"Overfitting\",\n",
    "    \"Backpropagation\",\n",
    "    \"Gradient_descent\",\n",
    "    \"Stochastic_gradient_descent\",\n",
    "    \"Feature_selection\",\n",
    "    \"Dimensionality_reduction\",\n",
    "    \"Clustering\",\n",
    "    \"Classification\",\n",
    "    \"Regression_analysis\",\n",
    "    \"Convolutional_neural_network\",\n",
    "    \"Recurrent_neural_network\",\n",
    "    \"Long_short-term_memory\",\n",
    "    \"Generative_adversarial_network\",\n",
    "    \"Autoencoder\",\n",
    "    \"Multilayer_perceptron\",\n",
    "    \"Perceptron\",\n",
    "    \"Activation_function\",\n",
    "    \"Batch_normalization\",\n",
    "    \"Residual_neural_network\",\n",
    "    \"Attention_(machine_learning)\",\n",
    "    \"Self-attention\",\n",
    "    \"Variational_autoencoder\",\n",
    "    \"Generative_artificial_intelligence\",\n",
    "    \"DALL-E\",\n",
    "    \"Midjourney\",\n",
    "    \"Stable_Diffusion\",\n",
    "    \"Diffusion_model\",\n",
    "    \"Optical_character_recognition\",\n",
    "    \"Object_detection\",\n",
    "    \"Facial_recognition_system\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5420375908487858,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f64921a4-a03a-48f0-ac95-82c0d2662400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define our ingestion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "824ff73b-c8e6-4960-baae-e9461fec362b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetch_wikipedia_article(title):\n",
    "    \"\"\"\n",
    "    Fetch Wikipedia article content using the Wikipedia API.\n",
    "    \n",
    "    Parameters:\n",
    "        title (str): The title of the Wikipedia article (e.g., \"Albert_Einstein\").\n",
    "    \n",
    "    Returns:\n",
    "        dict or None: A dictionary containing the title, full content, article URL, and fetch time,\n",
    "                      or None if the article couldn't be fetched.\n",
    "    \"\"\"\n",
    "    max_retries = 3\n",
    "    base_delay = 2\n",
    "    \n",
    "    # Add User-Agent header to be respectful to Wikipedia\n",
    "    headers = {\n",
    "        'User-Agent': 'Educational Research Bot 1.0 (contact@example.com)'\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            url = \"https://en.wikipedia.org/w/api.php\"\n",
    "            params = {\n",
    "                'action': 'query',\n",
    "                'format': 'json',\n",
    "                'titles': title,  # Use title as-is (the debug function works this way)\n",
    "                'prop': 'extracts',\n",
    "                'exintro': False,\n",
    "                'explaintext': True,\n",
    "                'exsectionformat': 'plain'\n",
    "            }\n",
    "            \n",
    "            # Make the request with headers and timeout (same as debug function)\n",
    "            response = requests.get(url, params=params, headers=headers, timeout=15)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Parse JSON response (same logic as debug function)\n",
    "                data = response.json()\n",
    "                pages = data['query']['pages']\n",
    "                page_id = list(pages.keys())[0]\n",
    "                \n",
    "                if page_id == '-1':\n",
    "                    # Article not found\n",
    "                    print(f\"Article '{title}' not found\")\n",
    "                    return None\n",
    "                \n",
    "                page_data = pages[page_id]\n",
    "                if 'extract' not in page_data:\n",
    "                    print(f\"No extract found for '{title}'\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        time.sleep(base_delay)\n",
    "                        continue\n",
    "                    return None\n",
    "                    \n",
    "                content = page_data['extract'].strip()\n",
    "                if not content:\n",
    "                    print(f\"Empty content for '{title}'\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        time.sleep(base_delay)\n",
    "                        continue\n",
    "                    return None\n",
    "                \n",
    "                # Return structured result (same as your original function)\n",
    "                return {\n",
    "                    'title': title,\n",
    "                    'content': content,\n",
    "                    'url': f\"https://en.wikipedia.org/wiki/{title}\",\n",
    "                    'fetch_time': datetime.now(),\n",
    "                    'content_length': len(content)\n",
    "                }\n",
    "            \n",
    "            elif response.status_code == 429:\n",
    "                # Rate limited - exponential backoff\n",
    "                wait_time = base_delay * (2 ** attempt) + random.uniform(0, 1)\n",
    "                print(f\"Rate limited (429) for {title}, waiting {wait_time:.1f}s before retry {attempt + 1}/{max_retries}\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                print(f\"HTTP {response.status_code} error for {title}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(base_delay)\n",
    "                    continue\n",
    "                return None\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout for {title} (attempt {attempt + 1}/{max_retries})\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(base_delay * (attempt + 1))\n",
    "                continue\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error for {title}: {type(e).__name__}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(base_delay)\n",
    "                continue\n",
    "            return None\n",
    "    \n",
    "    # All retries failed\n",
    "    print(f\"Failed to fetch {title} after {max_retries} attempts\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5420375908487858,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b41c9d18-7ded-46f0-bac3-c7298f45e35c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingest the Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fd1f851-b757-414b-bbdd-983148bcd97d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Fetch Wikipedia articles\n",
    "print(\"Fetching Wikipedia articles...\")\n",
    "articles_data = []\n",
    "\n",
    "for topic in WIKIPEDIA_TOPICS:\n",
    "    print(f\"Fetching: {topic}\")\n",
    "    article_data = fetch_wikipedia_article(topic)\n",
    "    if article_data and article_data['content']:\n",
    "        articles_data.append(article_data)\n",
    "        print(f\"✓ Successfully fetched {topic} ({len(article_data['content'])} characters)\")\n",
    "    else:\n",
    "        print(f\"✗ Failed to fetch {topic}\")\n",
    "\n",
    "print(f\"\\nSuccessfully fetched {len(articles_data)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5420375908487858,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0ccad29-d772-40b0-9c10-985bdca865ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Define a schema for the fetched articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8da0db87-3286-459f-99a2-c124e0046836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Create DataFrame from fetched articles\n",
    "schema = StructType([\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"content\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"fetch_time\", TimestampType(), True),\n",
    "    StructField(\"content_length\", LongType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5420375908487858,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a58da25-cd56-4dc8-a851-1a679f4d3287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Convert the articles to a Panda Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7972dab5-91ad-4406-95de-b20e15de7c27",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1753646756621}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame first, then to Spark DataFrame\n",
    "pandas_df = pd.DataFrame(articles_data)\n",
    "pandas_df.columns = ['title', 'content', 'url', 'fetch_time', 'content_length']\n",
    "\n",
    "df = spark.createDataFrame(pandas_df, schema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5420375908487858,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77ceadfd-12db-4509-a8ce-cf155343b60d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Extract/clean the content & split it into manageable chunks\n",
    "\n",
    "Steps:\n",
    "1. Define a function to clean Wikipedia text\n",
    "2. Define a function to split the text content into chunks\n",
    "3. Apply the functions to create a new DataFrame with the text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c7ec14c-4c0d-4949-9a99-cb35e4c4b498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Define a function to clean Wikipedia text\n",
    "def clean_wikipedia_text(text):\n",
    "    \"\"\"\n",
    "    Clean Wikipedia text by removing special formatting and references.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): Raw Wikipedia text content.\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text with references, section headers, and extra whitespace removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If the input text is None or empty, return an empty string\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove citation-style references like [1], [2], etc.\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    \n",
    "    # Replace multiple whitespace characters (spaces, tabs, newlines) with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove section headers enclosed in equal signs, e.g., \"== Heading ==\"\n",
    "    text = re.sub(r'={2,}.*?={2,}', '', text)\n",
    "    \n",
    "    # Trim leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5420375908487858,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49924615-e96f-4d2b-8df3-6851b47acd84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Define our Chunking Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31f14725-0b29-4151-a9f5-ebf34673eb6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Define a function to split the text content into chunks\n",
    "@pandas_udf(\"array<string>\")\n",
    "def read_as_chunk(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    # Set llama2 as tokenizer\n",
    "    set_global_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "    )\n",
    "    # Sentence splitter from llama_index to split on sentences\n",
    "    splitter = SentenceSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    \n",
    "    def process_batch(content_series):\n",
    "        result = []\n",
    "        for content in content_series:\n",
    "            if content:\n",
    "                # Clean the Wikipedia text\n",
    "                cleaned_content = clean_wikipedia_text(content)\n",
    "                \n",
    "                # Create Document object for llama_index\n",
    "                doc = Document(text=cleaned_content)\n",
    "                \n",
    "                # Split into chunks\n",
    "                chunks = splitter.split_text(cleaned_content)\n",
    "                \n",
    "                # Filter out very short chunks\n",
    "                valid_chunks = [chunk for chunk in chunks if len(chunk.strip()) > 100]\n",
    "                result.append(valid_chunks)\n",
    "            else:\n",
    "                result.append([])\n",
    "        \n",
    "        return pd.Series(result)\n",
    "    \n",
    "    for batch in batch_iter:\n",
    "        yield process_batch(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3850cbfe-29ea-4986-a6c5-90040b277bf6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1753652116321}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Apply the chunking function\n",
    "df_chunks = (\n",
    "    df.withColumn(\"chunks\", read_as_chunk(col(\"content\")))\n",
    "    .select(\"title\", \"url\", explode(\"chunks\").alias(\"content\"))\n",
    "    .filter(length(\"content\") > 100)  # Filter out very short chunks\n",
    ")\n",
    "\n",
    "df_chunks.display()\n",
    "print(f\"Created {df_chunks.count()} text chunks from Wikipedia articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b47e5e06-a110-4f63-bc1f-ee9f8f7e6fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Caculate Embeddings and store in a Delta Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0239a160-96dd-4d9a-8494-0c2241ab7ed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Define our get embedding UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "452e055a-8d8a-4ef5-a601-b47fb69bc6df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "def get_embedding_udf(batch_size=10):\n",
    "    \"\"\"\n",
    "    Returns a Spark UDF that takes a single text string and returns\n",
    "    a fixed-length float array (the embedding) by calling a deployed model.\n",
    "    \"\"\"\n",
    "    # Lazily import the MLflow Deployments API and get a client for Databricks\n",
    "    import mlflow.deployments\n",
    "    deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "    \n",
    "    def embed(text):\n",
    "        \"\"\"\n",
    "        Inner function that:\n",
    "        1) Handles empty or whitespace-only input by returning a zero-vector.\n",
    "        2) Calls the deployed embedding endpoint with the text.\n",
    "        3) Catches errors and falls back to returning a zero-vector.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # If text is None, empty, or only whitespace, return a default zero-vector\n",
    "            if not text or not str(text).strip():\n",
    "                return [0.0] * 1024\n",
    "            \n",
    "            # Call the embedding service via MLflow deployments\n",
    "            response = deploy_client.predict(\n",
    "                endpoint=\"databricks-bge-large-en\",   # your deployed endpoint name\n",
    "                inputs={\"input\": [str(text)]}         # wrap text in a list\n",
    "            )\n",
    "            \n",
    "            # Extract and return the embedding for the first (and only) input\n",
    "            return response.data[0]['embedding']\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log or print the error, then return a zero-vector to keep things flowing\n",
    "            print(f\"Error embedding text: {e}\")\n",
    "            return [0.0] * 1024\n",
    "    \n",
    "    # Wrap the `embed` function as a Spark UDF that returns ArrayType(FloatType())\n",
    "    return udf(embed, ArrayType(FloatType()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7831e7-c4ad-44b0-9043-a0558c68a4a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Invoke the UDF to calculate the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee8327c-7dc2-41dd-b84e-061cd96b97c8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1753652930983}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) Print a log so you know when embedding computation starts\n",
    "print(\"Computing embeddings for text chunks...\")\n",
    "\n",
    "# 2) Apply your embedding UDF to every row’s “content” and store result in a new column “embedding”\n",
    "df_chunk_emd = df_chunks.withColumn(\n",
    "    \"embedding\",\n",
    "    get_embedding_udf()(col(\"content\"))\n",
    ")\n",
    "\n",
    "# 3) In Databricks notebooks, `.display()` will render the DataFrame visually\n",
    "df_chunk_emd.display()\n",
    "\n",
    "# 4) Calling `.count()` actually runs the computation (including your UDF) \n",
    "#    and returns the total number of rows—then we log that number\n",
    "print(f\"Computed embeddings for {df_chunk_emd.count()} text chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "636d3a79-ab99-4e97-b13e-9f12073d3282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Store the embeddings in a Delta File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c5253d-a817-4009-aa6e-7c9cc6cdbf93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Steps:\n",
    "1. Create the Delta table schema\n",
    "2. Save the DataFrame containing the computed embeddings as a Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f2d030-571c-45b7-a78c-b8f2e9912a8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the Delta table\n",
    "embedding_table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.lab_wikipedia_text_embeddings\"\n",
    "\n",
    "drop_table_sql = f\"\"\"\n",
    "DROP TABLE IF EXISTS {embedding_table_name}\n",
    "\"\"\"\n",
    "spark.sql(drop_table_sql)\n",
    "print(f\"Dropped Delta table: {embedding_table_name}\")\n",
    "\n",
    "\n",
    "# SQL command to create the table\n",
    "create_table_sql = f\"\"\"\n",
    "CREATE TABLE  {embedding_table_name} (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  title STRING,\n",
    "  url STRING,\n",
    "  content STRING,\n",
    "  embedding ARRAY<FLOAT>\n",
    ") TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_table_sql)\n",
    "print(f\"Created Delta table: {embedding_table_name}\")\n",
    "\n",
    "## Save the DataFrame as a Delta table\n",
    "df_chunk_emd.write.mode(\"append\").saveAsTable(embedding_table_name)\n",
    "print(f\"Saved {df_chunk_emd.count()} records to Delta table\")\n",
    "\n",
    "## Verify the data was saved correctly\n",
    "verification_df = spark.sql(f\"SELECT COUNT(*) as total_records FROM {embedding_table_name}\")\n",
    "verification_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56b141d8-e855-4578-92dc-bd5a6e123bd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Query the Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83dcb87e-9247-49aa-ac75-9b1c7b124f2e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1753302811808}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM book_ai_ml_lakehouse.rag.lab_wikipedia_text_embeddings"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4963720176795107,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "09-02-RAG Data Preparation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
