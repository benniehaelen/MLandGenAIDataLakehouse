{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "ed584fc0-70f7-44a6-b1dd-6012e0f466f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display:flex; align-items:flex-start; margin-bottom:1rem;\">\n",
    "  <!-- Left: Book cover -->\n",
    "  <img\n",
    "    src=\"https://adb-1376134742576436.16.azuredatabricks.net/files/Images/book_cover.JPG\"\n",
    "    style=\"width:35%; margin-right:1rem; border-radius:4px; box-shadow:0 2px 6px rgba(0,0,0,0.1);\"\n",
    "    alt=\"Book Cover\"/>\n",
    "  <!-- Right: Metadata -->\n",
    "  <div style=\"flex:1;\">\n",
    "    <!-- O'Reilly logo above title -->\n",
    "    <div style=\"display:flex; flex-direction:column; align-items:flex-start; margin-bottom:0.75rem;\">\n",
    "      <img\n",
    "        src=\"https://cdn.oreillystatic.com/images/sitewide-headers/oreilly_logo_mark_red.svg\"\n",
    "        style=\"height:2rem; margin-bottom:0.25rem;\"\n",
    "        alt=\"O‘Reilly\"/>\n",
    "      <span style=\"font-size:1.75rem; font-weight:bold; line-height:1.2;\">\n",
    "        AI, ML and GenAI in the Lakehouse\n",
    "      </span>\n",
    "    </div>\n",
    "    <!-- Details, now each on its own line -->\n",
    "    <div style=\"font-size:0.9rem; color:#555; margin-bottom:1rem; line-height:1.4;\">\n",
    "      <div><strong>Name:</strong> 09-03-Vector Search</div>\n",
    "      <div><strong>Author:</strong> Bennie Haelen</div>\n",
    "      <div><strong>Date:</strong> 7-26-2025</div>\n",
    "    </div>\n",
    "    <!-- Purpose -->\n",
    "    <div style=\"font-weight:600; margin-bottom:0.75rem;\">\n",
    "      Purpose: This notebook demonstrates how to leverage a Mosaic Vector Search Endpoint in RAG\n",
    "    </div>\n",
    "    <!-- Outline -->\n",
    "    <div style=\"margin-top:0;\">\n",
    "      <h3 style=\"margin:0 0 0.25rem;\">Table of Contents</h3>\n",
    "      <ol style=\"padding-left:1.25rem; margin:0; color:#333;\">\n",
    "        <li>Fetch Wikipedia articles and load them into a DataFrame</li>\n",
    "        <li>Extract/clean the text content-split it into manageable chunks</li>\n",
    "        <li>Calculate the embeddings</li>\n",
    "        <li>Store the embeddings in a Delta file</li>\n",
    "      </ol>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b892de4-bfb7-45a4-861e-2bb608d779b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Pre-Requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62fdf403-8c11-4371-ac78-e2236faeedce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Install our requireed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee044e1e-1651-40df-87ff-08233936eae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qq databricks-vectorsearch databricks-sdk flashrank\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db4ca8cb-2633-4f42-8f6f-ac67513ffd26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Run our Common Code Notebook for this chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1621131-32fd-4b26-88a7-2e64132e0f8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run \"./9-00-Common-Code\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32013027-8b5f-46b7-8fe7-475c1fb3505a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Create our Vector Search Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39cb56f9-2613-4c30-b26b-79c1e6230774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Create the VectorSearchClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5427d9-b876-434d-93bc-8a1fa7a612fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "client = VectorSearchClient(disable_notice=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e4a36e-b9e0-4958-8ccc-7849ccb44c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b85ffa4-214a-4e17-ae6d-038bdf33fe2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This will try username-based naming first, then fallback if needed\n",
    "endpoint, endpoint_name = create_endpoint_with_fallback(\n",
    "    client=client,\n",
    "    username=USER_NAME,\n",
    "    endpoint_type=EndpointType.STANDARD,\n",
    "    wait_for_ready=True\n",
    ")\n",
    "\n",
    "print(f\"Created endpoint: {endpoint_name}\")\n",
    "\n",
    "full_endpoint_name = endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769b799a-90f9-4787-bebc-34e2f597688e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create a Vector Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f80ef9-0354-4910-91f6-c0b735c089dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " # Configuration parameters (replace with your actual values)\n",
    "ENDPOINT_NAME = full_endpoint_name\n",
    "\n",
    "# Table and index names\n",
    "source_table_name = \"lab_wikipedia_text_embeddings\"\n",
    "\n",
    "# where we want to store our index\n",
    "vs_index_name = \"lab_wikipedia-1\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OPTIMIZED VECTOR INDEX MANAGEMENT EXAMPLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    index = create_or_sync_vector_index(\n",
    "        client=client,\n",
    "        catalog_name=CATALOG_NAME,\n",
    "        schema_name=SCHEMA_NAME,\n",
    "        table_name=source_table_name,\n",
    "        index_name=vs_index_name,\n",
    "        endpoint_name=ENDPOINT_NAME,\n",
    "        primary_key=\"id\",\n",
    "        embedding_dimension=1024,  # Match your model embedding size (gte)\n",
    "        embedding_vector_column=\"embedding\",\n",
    "        pipeline_type=IndexPipelineType.TRIGGERED,\n",
    "        wait_for_ready=True,\n",
    "        max_wait_time=3600  # 1 hour timeout\n",
    "    )\n",
    "    print(\"✓ Index is ready for use!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Index management failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4476adbb-43ef-447e-9578-bd23872fc20f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Issuing Queries against the Vector Search Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7ccf725-6771-49eb-abf9-0797d9cafeff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Create an embeddings vector against our question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad0bfdf0-45f5-49c2-95de-d87363957f4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import MLflow deployments module for accessing deployed model endpoints\n",
    "import mlflow.deployments\n",
    "\n",
    "# Initialize MLflow deployment client for Databricks platform\n",
    "# This creates a connection to Databricks model serving infrastructure\n",
    "databricks_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "# Define the user's text query that will be converted to vector embeddings\n",
    "# This natural language question will be transformed into numerical representation\n",
    "user_query = \"Is Deep Learning the basis for Generative AI\"\n",
    "\n",
    "# Configuration for the embedding model endpoint\n",
    "embedding_model_endpoint = \"databricks-bge-large-en\"  # GTE large English model\n",
    "model_input_payload = {\"input\": [user_query]}\n",
    "\n",
    "# Call the GTE (General Text Embeddings) model endpoint to generate embeddings\n",
    "# - GTE model produces high-quality embeddings optimized for semantic similarity\n",
    "# - Input format: {\"input\": [list_of_text_strings]}\n",
    "# - Returns 1024-dimensional vectors that capture semantic meaning of the text\n",
    "embedding_response = databricks_client.predict(\n",
    "    endpoint=embedding_model_endpoint, \n",
    "    inputs=model_input_payload\n",
    ")\n",
    "\n",
    "# Extract the numerical embedding vectors from the model response\n",
    "# Response structure: response.data = [{\"embedding\": [float, float, ...]}, ...]\n",
    "# We extract only the vector arrays for downstream similarity search operations\n",
    "query_embeddings = [embedding_dict[\"embedding\"] for embedding_dict in embedding_response.data]\n",
    "\n",
    "# Display the generated embeddings for verification\n",
    "# These 1024-dimensional vectors are ready for similarity search in vector databases\n",
    "print(f\"Generated {len(query_embeddings)} embedding vector(s)\")\n",
    "print(f\"Embedding dimension: {len(query_embeddings[0]) if query_embeddings else 0}\")\n",
    "print(\"Embedding vectors:\", query_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c4758c7-662a-4d13-9634-a1460171f03a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(full_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aff38aa-f9ee-4c7f-98b4-4959c84781fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "full_index_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.lab_wikipedia-1\"\n",
    "print(f\"Full index name: {full_index_name}\")\n",
    "\n",
    "\n",
    "# get similar 5 documents.\n",
    "results = client.get_index(full_endpoint_name, full_index_name).similarity_search(\n",
    "  query_vector=query_embeddings[0],\n",
    "  columns=[\"title\", \"content\"],\n",
    "  num_results=5)\n",
    "\n",
    "import textwrap\n",
    "\n",
    "print(f\"\\nTop 5 results for: “{user_query}”\\n\" + \"=\"*60 + \"\\n\")\n",
    "rows = results.get(\"result\", {}).get(\"data_array\", [])\n",
    "print(f\"\\nTop {len(rows)} results for “{user_query}”\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for rank, row in enumerate(rows, start=1):\n",
    "    # row is a list with at least two items; guard against missing data\n",
    "    title   = row[0] if len(row) > 0 else \"<no title>\"\n",
    "    content = row[1] if len(row) > 1 else \"\"\n",
    "    snippet = textwrap.shorten(content, width=180, placeholder=\"…\")\n",
    "    \n",
    "    print(f\"\\n{rank:>2}. {title}\")\n",
    "    print(textwrap.indent(snippet, \"    \"))\n",
    "# # format result to align with reranker lib format. \n",
    "# passages = []\n",
    "# for doc in results.get(\"result\", {}).get(\"data_array\", []):\n",
    "#     new_doc = {\"file\": doc[0], \"text\": doc[1]}\n",
    "#     passages.append(new_doc)\n",
    "\n",
    "# print(passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d75cf9-2801-41a1-80c1-29902d48f51d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from flashrank import Ranker, RerankRequest\n",
    "\n",
    "# Initialize the reranker using the T5-based Flan model\n",
    "# (no need to specify cache_dir unless you want to control where Hugging Face stores weights)\n",
    "reranker = Ranker(model_name=\"rank-T5-flan\")\n",
    "\n",
    "# Prepare the list of passages for reranking.\n",
    "# Each passage is a dict containing:\n",
    "#  - \"text\": the content to be scored\n",
    "#  - \"file\": optional metadata (e.g., article title or source)\n",
    "passages = [\n",
    "    {\"text\": row[1], \"file\": row[0]}  # row[1] is the article content, row[0] is its title\n",
    "    for row in rows\n",
    "    if len(row) > 1                   # only include rows that have both title and content\n",
    "]\n",
    "\n",
    "# Build the rerank request with the user’s query and the passages\n",
    "request = RerankRequest(query=user_query, passages=passages)\n",
    "\n",
    "# Execute the reranking call; returns a list of results sorted by relevance\n",
    "reranked_results = reranker.rerank(request)\n",
    "\n",
    "# Iterate over the top 3 hits and print their details\n",
    "for i, hit in enumerate(reranked_results[:3], start=1):\n",
    "    score  = hit[\"score\"]                    # the relevance score (float)\n",
    "    text   = hit[\"text\"]                     # the passage text\n",
    "    source = hit.get(\"file\", \"<unknown source>\")  # the optional file/title metadata\n",
    "\n",
    "    # Output the formatted result\n",
    "    print(f\"Result #{i}\")\n",
    "    print(f\" Source: {source}\")\n",
    "    print(f\" Score : {score:.4f}\")\n",
    "    print(f\" Text  : {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ec66f81-d118-4844-8abf-f620bf5b3d8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Initialize the Databricks client (assumes your environment is authenticated)\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Create an OpenAI‑compatible client for Databricks Foundation Models\n",
    "openai_client = w.serving_endpoints.get_open_ai_client()\n",
    "\n",
    "# Assume reranked_results and user_query are already defined\n",
    "TOP_K = 3\n",
    "# Collect the top K passages with their source labels\n",
    "enriched_context = \"\\n\\n\".join(\n",
    "    f\"Source: {hit.get('file', '<unknown source>')}\\n{hit.get('text', '')}\"\n",
    "    for hit in reranked_results[:TOP_K]\n",
    ")\n",
    "\n",
    "# Build a concise prompt combining context and query\n",
    "generation_prompt = (\n",
    "    \"You are an expert assistant. Using only the provided context passages, \"\n",
    "    \"answer the user’s question accurately and concisely.\\n\\n\"\n",
    "    f\"{enriched_context}\\n\\n\"\n",
    "    f\"Question: {user_query}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "# Invoke the model via the Claude-compatible client\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"databricks-claude-3-7-sonnet\",\n",
    "    messages=[{\"role\": \"user\", \"content\": generation_prompt}]\n",
    ")\n",
    "\n",
    "# Extract and display the final answer\n",
    "final_answer = response.choices[0].message.content.strip()\n",
    "print(\"Final Answer:\\n\", final_answer)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "09-03-Vector Search",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
