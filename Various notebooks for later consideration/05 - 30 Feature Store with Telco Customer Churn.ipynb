{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef27dba3-1fd4-4ad3-ac46-671489764002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "#Handle Pre-Requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a160ea6-07c0-4ef8-8b94-4f6e4420a679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6757e80f-b275-46e5-be3a-0c3b4bdd3cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Make sure that kaggle and kagglehub are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2d74f4e-5ade-45b2-8e5b-4e9ff1cb4b02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e861b6e-ab89-4d22-8f06-92116fc4c702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b95f074d-bbb6-4f0e-a98b-c77dd51fd23b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5a50edf-ad12-4e89-9578-66261249c98a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Make sure to run the notebook with our constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de47a97a-ed23-4a91-9626-9f8a5161c26a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from databricks import feature_store\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2931f9eb-2c58-47a1-b8ac-faa3fbf3b312",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../common/Constants\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5432180b-cfc8-4366-83d4-d88659ee1f32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File locations\n",
    "TELCO_LOCAL_FILE_NAME = \"Telco_customer_churn.csv\"\n",
    "KAGGLE_FILE_LOCATION    = \"aadityabansalcodes/telecommunications-industry-customer-churn-dataset/versions/4\"\n",
    "\n",
    "# Table Name\n",
    "FEATURE_TABLE_NAME   = \"customer_churn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d023cc3e-a748-46e9-a4f7-7a0df7980c17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Use KaggleHub to download the Kaggle Dataset\n",
    "[Link to the dataset] https://www.kaggle.com/datasets/aadityabansalcodes/telecommunications-industry-customer-churn-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed7e1de2-8c5c-46fb-a125-c8d9517649b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Download the dataset to a local path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd17e8ee-1194-489d-9677-9e5beac7268f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the 'kagglehub' module to interact with Kaggle datasets.\n",
    "import kagglehub  \n",
    "\n",
    "# Download the latest version of the specified dataset.\n",
    "# 'dataset_download' takes the dataset identifier as an argument.\n",
    "# In this case, it downloads the dataset 'telecommunications-industry-customer-churn-dataset' by the user 'aadityabansalcodes',\n",
    "# Which is the Telco Customer Churn Dataset\n",
    "local_path = kagglehub.dataset_download(KAGGLE_FILE_LOCATION)\n",
    "\n",
    "# Print the local file path where the dataset files have been downloaded.\n",
    "print(\"Path to dataset files:\", local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9c994b4-7b4a-45db-a093-65be25353f88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Copy the local file to our DBFS datasets location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a0f11ce-4468-4c9f-934a-183aeed2081c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Construct the full local path by appending the file name to the existing local directory path\n",
    "local_path = f\"{local_path}/{TELCO_LOCAL_FILE_NAME}\"\n",
    "\n",
    "# Print the local path to verify correctness\n",
    "print(f\"The file has been downloaded to local path: {local_path}\")  \n",
    "\n",
    "# Define the DBFS path where you want to move the file\n",
    "# This path specifies where the file will be stored in the Databricks File System (DBFS)\n",
    "dbfs_path = f\"{DBFS_DATASET_DIRECTORY}/{TELCO_LOCAL_FILE_NAME}\"\n",
    "print(f\"The file will be copied to the dfbs location: {dbfs_path}\")\n",
    "\n",
    "# Use shutil.copy() to move the file from the local path to the DBFS path\n",
    "# This function copies the file to the specified DBFS directory, making it accessible to Databricks\n",
    "shutil.copy(local_path, dbfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ae93584-d59f-423f-9c40-39096dc9555f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls dbfs:/FileStore/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34964c6b-9a62-49e7-9e38-91634b5f6ee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f57000f-24ac-4541-b01d-5151cb72c472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if the path starts with '/dbfs'\n",
    "# The '/dbfs' prefix is used for local file system access, but Spark needs the path in DBFS format\n",
    "if dbfs_path.startswith(\"/dbfs\"):\n",
    "    dbfs_path = dbfs_path[5:]  # Remove the first 5 characters to strip the '/dbfs' prefix\n",
    "\n",
    "# Print the adjusted DBFS path to verify it has been modified correctly\n",
    "print(f\"Adjusted DBFS path: {dbfs_path}\")\n",
    "\n",
    "# Read the CSV file from the adjusted DBFS path using Spark\n",
    "# The 'header=True' option specifies that the first row of the file contains column names\n",
    "df = spark.read.csv(dbfs_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display the first 5 rows of the DataFrame to verify successful loading\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cae62020-f771-4504-9055-bae203a21611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Clean up the Column names (replace the spaces with underscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c777bc1-d57b-40dc-a60c-7adb6800ceeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for col_name in df.columns:\n",
    "    df = df.withColumnRenamed(col_name, col_name.replace(\" \", \"_\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfd7aeca-3404-477f-bd02-ce77dbfe0edb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Do some basic pre-processing\n",
    "Convert Total_Charges to numeric and handle errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0336983-5c80-4b3c-b6a2-d43846f1d973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "df = df.withColumn(\"Total_Charges\", regexp_replace(col(\"Total_Charges\"), \"[^0-9.]\", \"\"))\n",
    "df = df.withColumn(\"Total_Charges\", col(\"Total_Charges\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab2eb9ec-eca9-45e0-a420-df8d27f7cc1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS book_ai_ml_lakehouse.feature_store_db;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d336db8-5873-4736-adfa-f317d6221a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Start the Feature Store Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de308da6-0195-47a5-af33-8492c2888570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Select an initial set of features and write them to  a Feature Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae80870c-1b24-447e-afb7-74e6a5f3d3b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "\n",
    "# Feature Store Client\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "FEATURE_TABLE_NAME = \"telco_churn_demographic_features\"\n",
    "# Define the fully qualified feature table name (Unity Catalog)\n",
    "feature_table_name = f\"{CATALOG_NAME}.{FEATURE_STORE_DB}.{FEATURE_TABLE_NAME}\"\n",
    "print(f\"Feature table name: {feature_table_name}\")\n",
    "\n",
    "# Create an initial feature table with demographic features\n",
    "demo_features = df.select(\n",
    "    \"CustomerID\", \"Gender\", \"Senior_Citizen\", \"Partner\", \"Dependents\"\n",
    ")\n",
    "\n",
    "# Use the fully qualified name for the table creation\n",
    "fs.create_table(\n",
    "    name=feature_table_name,\n",
    "    primary_keys=[\"CustomerID\"],\n",
    "    schema=demo_features.schema,\n",
    "    description=\"Demographic features of customers\"\n",
    ")\n",
    "\n",
    "# Write the table using the fully qualified name\n",
    "fs.write_table(\n",
    "    name=feature_table_name,\n",
    "    df=demo_features,\n",
    "    mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82394010-03ac-4dd3-aa39-2292f02026de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create another feature table for service-related features\n",
    "service_features = df.select(\n",
    "    \"CustomerID\",\n",
    "    \"Phone_Service\",\n",
    "    \"Multiple_Lines\",\n",
    "    \"Internet_Service\",\n",
    "    \"Online_Security\",\n",
    "    \"Online_Backup\",\n",
    "    \"Device_Protection\",\n",
    "    \"Tech_Support\",\n",
    "    \"Streaming_TV\",\n",
    "    \"Streaming_Movies\"\n",
    ")\n",
    "\n",
    "FEATURE_TABLE_NAME = \"telco_churn_service_feature\"\n",
    "# Define the fully qualified feature table name (Unity Catalog)\n",
    "feature_table_name = f\"{CATALOG_NAME}.{FEATURE_STORE_DB}.{FEATURE_TABLE_NAME}\"\n",
    "\n",
    "fs.create_table(\n",
    "    name=feature_table_name,\n",
    "    primary_keys=[\"CustomerID\"],\n",
    "    schema=service_features.schema,\n",
    "    description=\"Service features of customers\"\n",
    ")\n",
    "fs.write_table(\n",
    "    name=feature_table_name,\n",
    "    df=service_features,\n",
    "    mode=\"overwrite\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbec493a-9168-4f89-9e7d-c9ba16f59c6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add new features using merge\n",
    "new_features = df.select(\n",
    "    \"CustomerID\",\n",
    "    \"Tenure_Months\",\n",
    "    \"Monthly_Charges\",\n",
    "    \"Total_Charges\"\n",
    ")\n",
    "\n",
    "FEATURE_TABLE_NAME = \"telco_churn_demographic_features\"\n",
    "# Define the fully qualified feature table name (Unity Catalog)\n",
    "feature_table_name = f\"{CATALOG_NAME}.{FEATURE_STORE_DB}.{FEATURE_TABLE_NAME}\"\n",
    "\n",
    "fs.write_table(\n",
    "    name=feature_table_name,\n",
    "    df=new_features,\n",
    "    mode=\"merge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09946673-622f-448b-8d51-6b075739706b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add or update rows in the feature store with custom values\n",
    "# Add or update rows in the feature store with custom values\n",
    "updated_rows = spark.createDataFrame([\n",
    "    (\"12345\", \"Female\", \"0\", \"Yes\", \"No\"),\n",
    "    (\"67890\", \"Male\", \"1\", \"No\", \"Yes\")\n",
    "], [\n",
    "    \"CustomerID\", \"Gender\", \"Senior_Citizen\", \"Partner\", \"Dependents\"\n",
    "])\n",
    "\n",
    "FEATURE_TABLE_NAME = \"telco_churn_demographic_features\"\n",
    "# Define the fully qualified feature table name (Unity Catalog)\n",
    "feature_table_name = f\"{CATALOG_NAME}.{FEATURE_STORE_DB}.{FEATURE_TABLE_NAME}\"\n",
    "\n",
    "fs.write_table(\n",
    "    name=feature_table_name,\n",
    "    df=updated_rows,\n",
    "    mode=\"merge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99d7ed0e-db24-414e-8fb0-6051160a1955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create a training set from the feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de68867-b588-4c58-a78b-50d2336ffbf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureLookup\n",
    "\n",
    "FEATURE_TABLE_NAME = \"telco_churn_demographic_features\"\n",
    "# Define the fully qualified feature table name (Unity Catalog)\n",
    "feature_table_name_demographic = f\"{CATALOG_NAME}.{FEATURE_STORE_DB}.{FEATURE_TABLE_NAME}\"\n",
    "\n",
    "FEATURE_TABLE_NAME = \"telco_churn_service_feature\"\n",
    "# Define the fully qualified feature table name (Unity Catalog)\n",
    "feature_table_name_churn_service = f\"{CATALOG_NAME}.{FEATURE_STORE_DB}.{FEATURE_TABLE_NAME}\"\n",
    "\n",
    "# Drop columns from the input DataFrame that overlap with feature store output\n",
    "overlapping_columns = [\n",
    "    \"Gender\", \"Senior_Citizen\", \"Partner\", \"Dependents\",\n",
    "    \"Tenure_Months\", \"Monthly_Charges\", \"Total_Charges\",\n",
    "    \"Phone_Service\", \"Multiple_Lines\", \"Internet_Service\",\n",
    "    \"Online_Security\", \"Online_Backup\", \"Device_Protection\",\n",
    "    \"Tech_Support\", \"Streaming_TV\", \"Streaming_Movies\"\n",
    "]\n",
    "df_cleaned = df.drop(*overlapping_columns)\n",
    "\n",
    "# Create a training dataset from the feature store\n",
    "training_set = fs.create_training_set(\n",
    "    df_cleaned,\n",
    "    feature_lookups=[\n",
    "        FeatureLookup(\n",
    "            table_name=feature_table_name_demographic,\n",
    "            lookup_key=\"CustomerID\"\n",
    "        ),\n",
    "        FeatureLookup(\n",
    "            table_name=feature_table_name_churn_service,\n",
    "            lookup_key=\"CustomerID\"\n",
    "        )\n",
    "    ],\n",
    "    label=\"Churn_Value\",\n",
    "    exclude_columns=[\"CustomerID\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fafbb08b-26aa-4dcf-9fb8-e17eb36ccdf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_df = training_set.load_df().toPandas()\n",
    "\n",
    "# Split the data\n",
    "X = training_df.drop(columns=[\"Churn_Value\"])\n",
    "y = training_df[\"Churn_Value\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64970d56-e084-4f82-aac7-07840dec107b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "# One-hot encoding for categorical columns\n",
    "X_train = pd.get_dummies(X_train, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# Align train and test sets\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1)\n",
    "X_test = X_test.fillna(0)\n",
    "\n",
    "# Ensure no missing values\n",
    "X_train = X_train.fillna(0)\n",
    "\n",
    "# Train the model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d103f5e-5838-4186-9be1-a76b4824a9a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from ydata_profiling import ProfileReport \n",
    "\n",
    "# Accuracy Score\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "\n",
    "# Generate a Confusion Matrix heatmap\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=clf.classes_, yticklabels=clf.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()\n",
    "\n",
    "# Generate a feature importance plot\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': clf.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances.head(10))\n",
    "plt.title(\"Top 10 Feature Importances\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Profiling Report\n",
    "print(\"Generating Profiling Report for Training Data...\")\n",
    "profile = ProfileReport(X_train, title=\"Training Data Report\", explorative=True)\n",
    "profile.to_file(\"training_data_report.html\")  # Opens the report as an HTML file\n",
    "\n",
    "# SHAP Analysis\n",
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary Plot\n",
    "shap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")\n",
    "\n",
    "# Detailed force plot for the first prediction\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][0, :], X_test.iloc[0, :])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6080560715861864,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "05 - 30 Feature Store with Telco Customer Churn",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
