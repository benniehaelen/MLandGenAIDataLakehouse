{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "d7a9b973-bd96-4de9-8f7c-f27440f1a988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src= \"https://cdn.oreillystatic.com/images/sitewide-headers/oreilly_logo_mark_red.svg\"/>&nbsp;&nbsp;<font size=\"16\"><b>AI, ML and GenAI in the Lakehouse<b></font></span>\n",
    "<img style=\"float: left; margin: 0px 15px 15px 0px; width:30%; height: auto;\" src=\"https://i.imgur.com/pQvJTVf.jpeg\"   />  \n",
    "\n",
    "\n",
    " \n",
    "  \n",
    "   Name:          chapter 06-01-Iris Feature Store Example\n",
    " \n",
    "   Author:    Bennie Haelen\n",
    "   Date:      02-09-2025\n",
    "\n",
    "   Purpose:   This notebook loads the \"lending club\" dataset, and converts it into a set of features, stored in the Feature Store. It then creates an ML model against those features.\n",
    "                 \n",
    "      An outline of the different sections in this notebook:\n",
    "        1 - Initialize the Feature Store Client\n",
    "        2 - Load and pre-processing the Lending Club Dataset\n",
    "              2-1 Load the Dataset\n",
    "              2-2 Add a unique ID to the Dataset\n",
    "              2-3 Perform Data Cleansing\n",
    "        3 - Perform Feature Engineering\n",
    "              3-1 Select our first sub-set of features\n",
    "              3-2 Make sure that our Feature Store Schema Exists.\n",
    "              3-3 Create the name of our Feature Table in the Unity Catalog\n",
    "              3-4 Write the initial set of features to the Feature Store.\n",
    "              3-5 Select another sub-set of Features\n",
    "              3-6 Merge in the Additional Features\n",
    "        4 - Prepare the Features for Model Generation\n",
    "              4-1 Read the table into a Pandas Dataframe.\n",
    "              4-2 Load the Target Variable\n",
    "              4-3 Merge the Features with the Labels\n",
    "              4-4 Drop rows with missing Target Values\n",
    "              4-5 Prepare Features for Model Training\n",
    "              4-6 Ensure all Features are Numeric\n",
    "              4-7 Impute Missing Values\n",
    "        5 - Build the model with the Feature Store Table Values\n",
    "              5-1 Split the dataset into Train and Test Datasets\n",
    "              5-2 Train a Random Forest Classifier\n",
    "              5-3 Make Predictions and Calculate the Model Accuracy\n",
    "        6 - Create Visualizations on the model\n",
    "              6-1 Create a Feature Importance Plot\n",
    "              6-2 Plot the Confusion Matrix\n",
    "              6-3 Visualize the distribution of the Target Variable\n",
    "              6-4 Plot the ROC Curve\n",
    "              6-5 Plot the Precision Recall curve\n",
    "        7 - Register the model in the Feature Store with MLflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee19deb0-ae60-4017-a271-4764ca2af53c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "#Handle Pre-Requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87d571a8-d773-4f24-bd67-b6bad6d88dfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Make sure to run the notebook with our constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf28326e-49a3-4d9b-bde8-d5fe6c9c2978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from databricks.feature_store import FeatureStoreClient,FeatureLookup\n",
    "from pyspark.sql.functions import col, when, row_number, monotonically_increasing_id, regexp_replace, trim\n",
    "from pyspark.sql.window import Window\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c67c8f-a29c-4b5e-b3e2-a31219f98272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../common/Constants\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b1bcbe7-0b10-4880-a1d3-2a58cdbc13e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Initialize the Feature Store Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4edf1f6a-8d4b-45ba-ae29-8d41f0c76e9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Feature Store client\n",
    "fs = FeatureStoreClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae130fb-2750-4a4b-8192-31e06b77dd91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Load and preprocess the Lending Club Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e545571-6e4e-404e-bfb2-35ec6eb40a6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eaa2d6d-2314-4adf-b8da-007d896287a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load and Preprocess the Dataset with feature engineering\n",
    "# Load the Lending Club Loan dataset\n",
    "# The dataset is stored in Parquet format in Databricks' public datasets.\n",
    "# This dataset contains information on issued loans, including features\n",
    "# like loan amount, interest rate, and borrower details.\n",
    "lending_club_path = \"/databricks-datasets/samples/lending_club/parquet\"\n",
    "df = spark.read.parquet(lending_club_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e3eac40-aed6-4b24-b671-d3852b86d076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Add a Unique ID to the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5384c2db-8f3f-4589-8b92-dcec12a7ce21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate a globally unique ID for each row\n",
    "# The monotonically_increasing_id() function generates a unique, \n",
    "# monotonically increasing 64-bit integer for each row.\n",
    "# This ID is used as a primary key to ensure that each record is \n",
    "# uniquely identifiable in the Feature Store.\n",
    "df = df.withColumn(\"unique_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb3ee96-ce60-4028-bbc0-3e99351d6ced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31bf1bee-112e-4e60-bf72-a9d6cd33c5b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean percentage fields by removing '%' and converting to float\n",
    "# The 'int_rate' and 'revol_util' columns contain percentage values \n",
    "# stored as strings (e.g., '13.56%').\n",
    "# The regexp_replace() function removes the '%' symbol, and \n",
    "# the resulting string is cast to a double for numerical analysis.\n",
    "# The trim() function is used to remove any leading or \n",
    "# trailing whitespace before conversion.\n",
    "df = df.withColumn(\"int_rate\", regexp_replace(trim(col(\"int_rate\")), \"%\", \"\").cast(\"double\"))\n",
    "df = df.withColumn(\"revol_util\", regexp_replace(trim(col(\"revol_util\")), \"%\", \"\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c4f9cad-b1b1-4d70-8e4a-6973ba1cb4d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Perform Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d15d401-0e86-4531-983b-446292e1692f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Select our first sub-set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b4153a1-f368-4b8f-92e9-62de61d6480d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Selecting relevant features for the model.\n",
    "# The 'unique_id' column serves as the primary key for uniquely \n",
    "# identifying each row in the Feature Store.\n",
    "initial_features_df = df.select(\n",
    "    col(\"unique_id\"),   # Unique identifier for each loan record\n",
    "    col(\"loan_amnt\"),   # The amount of money requested by the borrower\n",
    "    col(\"int_rate\"),    # Interest rate of the loan, converted to numeric format\n",
    "    col(\"annual_inc\"),  # The Borrower's annual income\n",
    "    col(\"dti\"),         # Debt-to-income ratio, a key metrics for credit risk\n",
    "    col(\"revol_util\"),  # Revolving credit line utilization rate\n",
    "    col(\"open_acc\")     # Number of open credit lines in the borrow's credit file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14fb7494-1643-48a0-86bf-b3e79737a40c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Make sure that our Feature Store Schema Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aca188d-d9b9-4e59-8beb-8bc12c1685eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS book_ai_ml_lakehouse.feature_store_db;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ff0502-044f-4827-8905-e216a65df394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Create the name of our Feature Table in the Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3b34b7d-cc11-4efd-9cba-8d1d1cf7b849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We will be using Unity Catalog to store our Feature Table.\n",
    "# With Unity Catalog, we specify the fully qualified name to ensure\n",
    "# consistent governance and access control across all workspaces.\n",
    "# The format for the name is: 'catalog.schema.table_name'. \n",
    "FEATURE_TABLE_NAME = \"lending_club_loan\"\n",
    "feature_table_name = f\"{CATALOG_NAME}.{FEATURE_STORE_DB}.{FEATURE_TABLE_NAME}\"\n",
    "print(f\"Our feature table will be named: `{feature_table_name}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38d2f1f4-2bd1-460a-9bef-f98cd023455d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Write the initial set of features to the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "806f65d3-ea4d-47ce-91f3-6ac510feaa46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fs.drop_table(feature_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3da87d4e-a149-4754-8f66-058a0d628f1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the initial subset of features to the Databricks Feature Store\n",
    "fs.create_table(\n",
    "    name=feature_table_name,     # Fully qualified name recommended if using Unity Catalog for centralized governance\n",
    "    primary_keys=[\"unique_id\"],  # 'unique_id' ensures each record in the feature store is uniquely identifiable\n",
    "    df=initial_features_df,      # DataFrame containing the initial set of engineered features\n",
    "    description=\"Initial subset of engineered features for Lending Club Loan dataset\"  # Descriptive metadata to help with dataset management and discovery\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cacce56-a493-45f1-98cf-4dfd8fe8b6a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Select another sub-set of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed46d536-e856-4e96-9475-536650f39311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Additional feature engineering\n",
    "# Selecting additional relevant features to enhance model performance.\n",
    "# These features provide deeper insights into the borrower's credit history and financial health.\n",
    "# Loading additional features into a PySpark DataFrame\n",
    "additional_features_df = spark.createDataFrame(\n",
    "    df.select(\n",
    "        col(\"unique_id\"),    # Unique identifier to maintain consistency with the primary key in the feature store\n",
    "        col(\"delinq_2yrs\"),  # Number of 30+ days past-due incidences in the borrower's credit file in the last 2 years\n",
    "        col(\"pub_rec\"),      # Number of derogatory public records (e.g., bankruptcies, liens)\n",
    "        col(\"total_acc\"),    # Total number of credit lines in the borrower's credit file\n",
    "        col(\"mort_acc\")      # Number of mortgage accounts the borrower has\n",
    "    ).rdd  # Convert the selected columns to an RDD before creating the DataFrame\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6359f81d-4053-44bc-be18-8f1ea0605166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Merge in the Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "118d6f78-a4dd-4b10-a5d1-c391c5d383c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Appending additional features to the Feature Store\n",
    "# This step integrates the newly engineered features into \n",
    "# the existing feature table in the Feature Store.\n",
    "# By using 'merge' mode, the new features are appended without \n",
    "# overwriting the existing data, ensuring historical data integrity.\n",
    "# Unity Catalog governs the append operation, enforcing consistent \n",
    "# access control and ensuring data lineage is maintained across updates.\n",
    "# Unity Catalog ensures that appending features maintains data \n",
    "# consistency and adheres to organization-wide access policies.\n",
    "# Using 'merge' mode here allows for seamless integration of new \n",
    "# features while preserving existing data integrity.\n",
    "fs.write_table(\n",
    "    name=feature_table_name,    # Fully qualified feature table name ensures proper governance if Unity Catalog is enabled\n",
    "    df=additional_features_df,  # DataFrame containing the newly engineered features\n",
    "    mode=\"merge\"  # 'merge' mode appends new features while preserving existing records\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea936b6d-ad75-4e9a-8d27-8f07cc353757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Prepare the Features for Model Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2312e70f-7556-4cc4-b627-c4db9bd6a866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Reading Features from the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "295a3b71-7802-4d98-8f13-b9d43f0a082e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading features from the Feature Store\n",
    "# Features are read from the Feature Store and converted into a Pandas DataFrame for local processing.\n",
    "features = fs.read_table(feature_table_name).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba791a66-4e43-4364-8efe-494bae758284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Load the Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c7322d-3e3b-4479-984f-57f90157de44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load target variable (e.g., predicting loan default)\n",
    "# The target variable 'defaulted' is created based on the 'loan_status' column.\n",
    "# If the loan status is 'Charged Off', it indicates a default, and the value \n",
    "# is set to True; otherwise, it is False.\n",
    "# The 'unique_id' ensures the target variable aligns with the features \n",
    "# in the Feature Store for proper model training and evaluation.\n",
    "# The data is then converted to a Pandas DataFrame for compatibility with scikit-learn.\n",
    "labels = df.select(\"unique_id\", (col(\"loan_status\") == \"Charged Off\").alias(\"defaulted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b969b23-6147-4872-9095-cdfcd9fb9258",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Merging the Features with the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a1a877b-caf9-48dd-8dce-fb5187c0928f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge features with labels\n",
    "# Combines the engineered features from the Feature Store \n",
    "# with the target variable 'defaulted'.\n",
    "# The merge is performed using 'unique_id' to ensure each feature set \n",
    "# is accurately linked to its corresponding target label.\n",
    "data = pd.merge(features, labels.toPandas(), on=\"unique_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9229a2e5-a1f6-4ec7-a34e-c88e475ff87b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c1da8d0-e028-42f5-92d0-7d7f1114d205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Drop rows with missing Target Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55de1302-900b-4dc1-b5d0-4beea0d57b8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing target values\n",
    "# Ensures that any records without a defined 'defaulted' status are removed.\n",
    "# This step is crucial because machine learning models cannot train on missing target labels.\n",
    "data = data.dropna(subset=[\"defaulted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "716708ca-361b-4979-8d69-71e8f3d53179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Separating the Features from the Target Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af4da7c9-1095-48c6-a491-abe279a785da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare features for model training\n",
    "# Drops 'unique_id' (as it's not useful for prediction) and 'defaulted' (the target variable).\n",
    "# This ensures the model only trains on relevant feature columns.\n",
    "X = data.drop(columns=[\"unique_id\", \"defaulted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc875054-1e6a-4925-8154-a85b03ff199e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Ensure all Features are Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4981d4ef-03ca-4df0-a09c-e9fa73c4e7c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure all features are numeric\n",
    "# Converts all feature columns to numeric types, coercing errors to NaN if necessary.\n",
    "# This is essential because machine learning algorithms require numeric input.\n",
    "X = X.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01802968-681a-4d7d-bb28-7a1e1c93b4a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ecff421-746d-44b6-be56-1608520270ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdaf7e68-5089-49e8-8e19-1d7ef50fb6cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Impute missing values using mean strategy\n",
    "# The SimpleImputer replaces missing values (NaNs) in the feature set with the mean of each column.\n",
    "# This is a common preprocessing step to handle incomplete data, ensuring that machine learning algorithms\n",
    "# can be trained without errors due to missing values.\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "210d5ecb-b4f9-4e25-9aed-b5e78261956e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the Target variable\n",
    "y = data[\"defaulted\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa0de175-0e25-4130-969b-0dc53380c668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Build the model with the Features from the Feature Store Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8954942b-ae0d-4829-9a72-01eb86a90972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Split the dataset into Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73977482-eda3-41c6-a5c4-1cafed7989e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "# Divides the dataset into training and testing sets to evaluate model performance.\n",
    "# 80% of the data is used for training, and 20% is reserved for testing.\n",
    "# The 'random_state' ensures reproducibility by setting a seed for the random number generator.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4189631b-628b-430f-9acb-aa72e044397a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Train a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b467b405-12a1-4a41-8731-819bd54a07c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train a Random Forest classifier\n",
    "# Initializes the Random Forest model with 100 decision trees (estimators).\n",
    "# The 'random_state' parameter ensures reproducibility by fixing the random number generation.\n",
    "# The model is then trained using the training feature set (X_train) and target labels (y_train).\n",
    "# The target variable is explicitly converted to integers to ensure compatibility with the classifier.\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train.astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d547362-2643-4d80-8a05-af47cea46c17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Make Predictions and Calculate the Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00bf3161-4aff-40e8-86e2-637f4c0dd994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "# Ensure target labels are binary and consistent\n",
    "# Converts the target labels in y_test to integers if they are in boolean format.\n",
    "# This step is crucial because scikit-learn's classification metrics expect numerical labels (0 and 1).\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "# Uses the trained Random Forest model to predict loan default outcomes on the test set.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Check for unexpected values\n",
    "# Prints the unique values in y_test and y_pred to ensure both contain only binary values (0 and 1).\n",
    "# This step helps identify any anomalies or data inconsistencies that could affect evaluation.\n",
    "print(\"Unique values in y_test:\", y_test.unique())\n",
    "print(\"Unique values in y_pred:\", pd.Series(y_pred).unique())\n",
    "\n",
    "# Evaluate model accuracy\n",
    "# Calculates the accuracy of the model by comparing predicted values (y_pred) with actual values (y_test).\n",
    "# Accuracy represents the proportion of correct predictions out of all predictions made.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2383cb47-0fcf-4a64-84e7-ccd83cd57d87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Infer the signature, we can use it later with model registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf4d4a0-0df7-4312-b303-60ea9dbc8d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Infer model signature for Unity Catalog\n",
    "signature = infer_signature(X_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efc1bb05-1889-4240-a603-03b01f9f9d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Create Visualizations on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fc176df-bdc9-4602-b159-ae43406212da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Create a Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20adba62-00ea-4a1d-9db6-de6d191296fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract feature importances from the trained model\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame with feature names and their corresponding importance scores\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,  # X.columns gives the names of your features\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the features by importance (highest to lowest) for better visualization\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3281fb3-e1c3-44a1-8103-133e518128c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure 'importance_df' exists and is correctly structured\n",
    "# Example: importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': model.feature_importances_})\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Apply a clean, modern style for publication\n",
    "sns.set_style(\"whitegrid\")  # Add a white grid background\n",
    "sns.set_context(\"notebook\", font_scale=1.2)  # Increase font size for better visibility\n",
    "\n",
    "# Sort features by importance for better visualization\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "ax = sns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis', edgecolor='black')\n",
    "\n",
    "# Enhance plot aesthetics\n",
    "ax.set_title('Feature Importances in Random Forest Model', fontsize=16, weight='bold')\n",
    "ax.set_xlabel('Importance Score', fontsize=12)\n",
    "ax.set_ylabel('Feature', fontsize=12)\n",
    "\n",
    "# Add data labels on bars for clarity\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    # Add a small offset to prevent overlap with the bar edge\n",
    "    ax.annotate(f'{width:.3f}', (width + 0.01, p.get_y() + p.get_height() / 2),\n",
    "                ha='left', va='center', fontsize=10, color='black', weight='bold')\n",
    "\n",
    "# Remove top and right spines for a cleaner look\n",
    "sns.despine()\n",
    "\n",
    "# Ensure layout fits well\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd0b3d1b-96e3-47e8-8902-dcf4be50f394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Plot the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70de48f5-9cff-4119-be42-a0c0e343acb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "\n",
    "# Generate synthetic data for demonstration\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Visualize predictions against actuals\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Apply a clean, modern style for the plot\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# Create a confusion matrix to compare predictions and actuals\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=[\"Fully Paid\", \"Defaulted\"])\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "cmd.plot(cmap='coolwarm', values_format='d', ax=ax, colorbar=False)\n",
    "\n",
    "# Enhance aesthetics by removing grid lines and adding custom formatting\n",
    "ax.set_title('Confusion Matrix: Predictions vs Actuals', fontsize=16, weight='bold')\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label', fontsize=12)\n",
    "\n",
    "# Remove the grid lines for a cleaner look\n",
    "ax.grid(False)\n",
    "\n",
    "# Add black borders to each square for better separation\n",
    "for text in ax.texts:\n",
    "    text.set_color('black')  # Ensure text is black for better contrast\n",
    "\n",
    "# Adjust tick labels for clarity\n",
    "ax.xaxis.set_tick_params(width=0)\n",
    "ax.yaxis.set_tick_params(width=0)\n",
    "\n",
    "# Remove spines for a more minimalistic appearance\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Create a DataFrame for metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Score': [accuracy, precision, recall, f1]\n",
    "})\n",
    "\n",
    "# Display the metrics table\n",
    "plt.figure(figsize=(6, 2))\n",
    "plt.axis('off')\n",
    "table = plt.table(cellText=metrics_df.values, colLabels=metrics_df.columns, cellLoc='center', loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.scale(1, 1.5)\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07c6ca04-6052-401a-aaa6-117423264191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Visualize the distribution of the Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cee02297-5ab8-40f4-97d5-7f05226551c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of the target variable\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Create a count plot with a modern, clean style\n",
    "sns.set_style(\"whitegrid\")  # Apply a white grid background for better readability\n",
    "sns.set_context(\"notebook\", font_scale=1.2)  # Increase font size for better visibility in print\n",
    "\n",
    "# Plot the distribution of loan defaults\n",
    "ax = sns.countplot(x='defaulted', data=data, palette='coolwarm', edgecolor='black')\n",
    "\n",
    "# Enhance plot aesthetics\n",
    "ax.set_title('Distribution of Loan Defaults', fontsize=16, weight='bold')\n",
    "ax.set_xlabel('Loan Status (0 = Fully Paid, 1 = Defaulted)', fontsize=12)\n",
    "ax.set_ylabel('Number of Loans', fontsize=12)\n",
    "\n",
    "# Add data labels on bars for clarity\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.annotate(f'{height}', (p.get_x() + p.get_width() / 2., height), \n",
    "                ha='center', va='bottom', fontsize=10, color='black', weight='bold')\n",
    "\n",
    "# Remove top and right spines for a cleaner look\n",
    "sns.despine()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f6a6aec-7ce3-4bf4-88c2-e60922854d74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Log the model in the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f744d20-c6f6-4575-bf85-4c5450908c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The unique identifier for the MLflow run where the model was logged.\n",
    "# MLflow assigns a unique 'run_id' to each experiment run. This ID is critical \n",
    "# for tracking the specific model version, its parameters, metrics, and artifacts.\n",
    "run_id = \"3cb01eb8044e47feb5d9dcaedbfc9fcd\"\n",
    "\n",
    "# The name assigned to the model when registering it in MLflow.\n",
    "# This 'model_name' serves as a reference for the registered model in both MLflow \n",
    "# and Unity Catalog, enabling consistent access across different environments.\n",
    "model_name = \"lending_club_default_model\"\n",
    "\n",
    "# The catalog name in Unity Catalog, representing the top-level namespace.\n",
    "# Unity Catalog uses 'catalog_name' as the primary organizational layer \n",
    "# for managing data and models across the lakehouse.\n",
    "catalog_name = CATALOG_NAME  # Ensure CATALOG_NAME is defined earlier in your code.\n",
    "\n",
    "# The schema name within the catalog, used for organizing registered models.\n",
    "# The 'schema_name' helps categorize models within a specific catalog, \n",
    "# promoting better governance and discoverability.\n",
    "schema_name = FEATURE_STORE_DB  # Ensure FEATURE_STORE_DB is defined earlier in your code.\n",
    "\n",
    "# Construct the fully qualified Unity Catalog model name.\n",
    "# The format 'catalog.schema.model_name' ensures that the model is uniquely \n",
    "# identifiable and governed across workspaces in Databricks.\n",
    "unity_model_name = f\"{catalog_name}.{schema_name}.{model_name}\"\n",
    "\n",
    "# Display the full model path in Unity Catalog for verification.\n",
    "# This helps confirm that the model has been correctly registered \n",
    "# and can be easily referenced for deployment or inference.\n",
    "print(f\"Unity Model Name: {unity_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d964f8da-1976-4256-a988-114d61932d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the registry URI to Databricks Unity Catalog.\n",
    "# By default, MLflow uses its internal model registry for tracking models. \n",
    "# This command overrides the default and instructs MLflow to use Databricks Unity Catalog \n",
    "# as the backend for model management.\n",
    "#\n",
    "# Unity Catalog provides centralized governance, lineage tracking, and access control \n",
    "# for models across all Databricks workspaces. By integrating MLflow with Unity Catalog, \n",
    "# models benefit from enhanced security, consistent data governance, and \n",
    "# cross-workspace discoverability.\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769bcb34-c2d2-436c-a5de-017e9da7a251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient \n",
    "\n",
    "# Start an MLflow run to log model artifacts and metrics\n",
    "# This context manager initializes a new MLflow run, allowing you to log models, metrics, \n",
    "# and other artifacts. The run provides a unique run ID that helps track and reference \n",
    "# the model within the MLflow and Unity Catalog ecosystems.\n",
    "with mlflow.start_run() as run:\n",
    "    \n",
    "    # Log the trained Random Forest model along with its signature\n",
    "    # The model is logged using MLflow's sklearn integration. The 'signature' ensures \n",
    "    # that the input-output schema of the model is recorded, maintaining consistency \n",
    "    # between training and inference.\n",
    "    mlflow.sklearn.log_model(clf, \"model\", signature=signature)\n",
    "    \n",
    "    # Log the model's accuracy metric\n",
    "    # This captures the model's accuracy as a performance metric in MLflow, \n",
    "    # allowing easy tracking, comparison, and evaluation across different experiments.\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Register the model in Unity Catalog\n",
    "    # This step registers the logged model in Unity Catalog for centralized governance \n",
    "    # and cross-workspace accessibility. The 'run.info.run_id' links this registration \n",
    "    # to the specific MLflow run, ensuring full traceability.\n",
    "    model_version = mlflow.register_model(f\"runs:/{run.info.run_id}/model\", unity_model_name)\n",
    "\n",
    "    # Define feature lookups for model deployment\n",
    "    # The FeatureLookup specifies which features the model requires at inference time. \n",
    "    # It maps the 'unique_id' key to features stored in the Feature Store, ensuring \n",
    "    # consistency between training and serving.\n",
    "    feature_lookups = [\n",
    "        FeatureLookup(\n",
    "            table_name=feature_table_name,  # Name of the feature table in Unity Catalog\n",
    "            feature_names=[\"loan_amnt\", \"int_rate\", \"annual_inc\", \"dti\", \"revol_util\", \"open_acc\"],  # Features used in the model\n",
    "            lookup_key=\"unique_id\"  # Primary key used for joining features during inference\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Link the model to the Feature Store for feature consistency during inference\n",
    "    # This step logs the model in both MLflow and the Feature Store, associating it \n",
    "    # with the features used during training. The 'training_set' ensures that the model \n",
    "    # remains consistent with the feature data, enabling reproducibility and \n",
    "    # simplifying deployment workflows.\n",
    "    fs.log_model(\n",
    "        model=clf,  # The trained Random Forest model\n",
    "        artifact_path=\"model\",  # Path within the MLflow run where the model is stored\n",
    "        flavor=mlflow.sklearn,  # Specifies the MLflow flavor for scikit-learn models\n",
    "        training_set=fs.create_training_set(\n",
    "            df=df.select(\"unique_id\", (col(\"loan_status\") == \"Charged Off\").alias(\"defaulted\")),  # Dataset with labels\n",
    "            feature_lookups=feature_lookups,  # Defined feature lookups for inference\n",
    "            label=\"defaulted\"  # Target variable used during training\n",
    "        ),\n",
    "        registered_model_name=unity_model_name  # Fully qualified model name in Unity Catalog\n",
    "    )\n",
    "\n",
    "# Confirm successful model training, logging, and registration\n",
    "print(\"Model training and logging to Unity Catalog completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67ad4308-7961-4972-b3dd-61af63eb8ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#End of Notebook"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6990078474652937,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "06 - 01 Lending Club Feature Store Example",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
