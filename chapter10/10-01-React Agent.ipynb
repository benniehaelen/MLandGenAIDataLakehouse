{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "3fb87915-b037-4589-b710-4ce8f723fe74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display:flex; align-items:flex-start; margin-bottom:1rem;\">\n",
    "  <!-- Left: Book cover -->\n",
    "  <img\n",
    "    src=\"https://i.imgur.com/ITL8dZE.jpeg\"\n",
    "    style=\"width:35%; margin-right:1rem; border-radius:4px; box-shadow:0 2px 6px rgba(0,0,0,0.1);\"\n",
    "    alt=\"Book Cover\"/>\n",
    "  <!-- Right: Metadata -->\n",
    "  <div style=\"flex:1;\">\n",
    "    <!-- O'Reilly logo above title -->\n",
    "    <div style=\"display:flex; flex-direction:column; align-items:flex-start; margin-bottom:0.75rem;\">\n",
    "      <img\n",
    "        src=\"https://cdn.oreillystatic.com/images/sitewide-headers/oreilly_logo_mark_red.svg\"\n",
    "        style=\"height:2rem; margin-bottom:0.25rem;\"\n",
    "        alt=\"O‚ÄòReilly\"/>\n",
    "      <span style=\"font-size:1.75rem; font-weight:bold; line-height:1.2;\">\n",
    "        AI, ML and GenAI in the Lakehouse\n",
    "      </span>\n",
    "    </div>\n",
    "    <!-- Details, now each on its own line -->\n",
    "    <div style=\"font-size:0.9rem; color:#555; margin-bottom:1rem; line-height:1.4;\">\n",
    "      <div><strong>Name:</strong>10-01-Financial Agents</div>\n",
    "      <div><strong>Author:</strong> Bennie Haelen</div>\n",
    "      <div><strong>Date:</strong> 7-26-2025</div>\n",
    "    </div>\n",
    "    <!-- Purpose -->\n",
    "    <div style=\"font-weight:600; margin-bottom:0.75rem;\">\n",
    "      Purpose: This notebook contains the code for the React Notebook\n",
    "    </div>\n",
    "    <!-- Outline -->\n",
    "    <div style=\"margin-top:0;\">\n",
    "      <h3 style=\"margin:0 0 0.25rem;\">Table of Contents</h3>\n",
    "      <ol style=\"padding-left:1.25rem; margin:0; color:#333;\">\n",
    "        <li>Fetch Wikipedia articles and load them into a DataFrame</li>\n",
    "        <li>Extract/clean the text content-split it into manageable chunks</li>\n",
    "        <li>Calculate the embeddings</li>\n",
    "        <li>Store the embeddings in a Delta file</li>\n",
    "      </ol>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a31c467f-cd1d-4bb0-a845-ee90acfe2222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Notebook Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "612c3d4e-a5ec-4581-a0cf-0a094dea2c85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be4d2b41-816d-4682-9c7d-cef9e6ee516d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For full agent mode  \n",
    "!pip install --upgrade langchain langchain-openai pandas \n",
    " \n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f0e82f-05bc-4f9d-b34c-fb8a569af60d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eccddbc-aede-4014-abfb-37a337cc21bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "React Agent for Databricks Lakehouse Intelligence\n",
    "Author: Bennie Haelen\n",
    "Book: AI and ML in a Lakehouse\n",
    "\n",
    "This Notebook provides a complete, example of a React agent designed \n",
    "to interact with a Databricks Lakehouse. It uses the Databricks\n",
    "Connect library to query data, includes multiple tools for analysis, and\n",
    "features both a live and a demo mode for educational purposes.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "# LangChain components for building the agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.callbacks import CallbackManagerForToolRun\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "\n",
    "# Databricks Connect for querying the Lakehouse\n",
    "from databricks.connect import DatabricksSession\n",
    "from databricks.sdk.core import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bccf1e36-4e71-439d-8a01-e5ea50aec698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Perform MLflow Autologging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560474f0-3160-4d25-9e9f-1017db8eeca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e46d8dd-e0ff-4411-a4b8-165e499776da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Setup the Lakehouse Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc34ef08-3bad-405d-9bb0-f7a972566e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Utility Function to Set Up the Environment ---\n",
    "\n",
    "def setup_lakehouse_environment():\n",
    "    \"\"\"\n",
    "    Creates the necessary schemas and tables in the Databricks environment\n",
    "    and populates them with sample data. This function makes the example\n",
    "    self-contained and runnable.\n",
    "    \"\"\"\n",
    "    print(\"Setting up the Databricks Lakehouse environment...\")\n",
    "    # spark = DatabricksSession.builder.getOrCreate()\n",
    "\n",
    "    # Dynamically get the current catalog to ensure portability\n",
    "    current_catalog = spark.catalog.currentCatalog()\n",
    "    print(f\"Using current catalog: '{current_catalog}'\")\n",
    "\n",
    "    # Define schemas within the current catalog\n",
    "    schemas = [f\"{current_catalog}.bronze\", f\"{current_catalog}.silver\", f\"{current_catalog}.gold\"]\n",
    "    for schema_name in schemas:\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "    print(\"Schemas created successfully.\")\n",
    "\n",
    "    # --- Create and Populate Tables ---\n",
    "\n",
    "    # GOLD LAYER\n",
    "    gold_data = {\n",
    "        'month': pd.to_datetime(['2025-07-31', '2025-06-30', '2025-05-31']),\n",
    "        'avg_engagement_premium': [71.0, 83.0, 85.0],\n",
    "        'avg_engagement_standard': [60.0, 62.0, 64.0],\n",
    "        'total_revenue': [250000.0, 245000.0, 240000.0],\n",
    "        'premium_customer_count': [1050, 1045, 1040]\n",
    "    }\n",
    "    gold_df = pd.DataFrame(gold_data)\n",
    "    spark.createDataFrame(gold_df).write.mode(\"overwrite\").saveAsTable(f\"{current_catalog}.gold.monthly_engagement\")\n",
    "\n",
    "    # SILVER LAYER\n",
    "    silver_customers_data = {\n",
    "        'customer_id': range(101, 111),\n",
    "        'tier': ['premium', 'standard'] * 5,\n",
    "        'signup_date': pd.to_datetime(pd.date_range('2024-01-01', periods=10)),\n",
    "        'lifetime_value': [500 + i*100 for i in range(10)]\n",
    "    }\n",
    "    silver_customers_df = pd.DataFrame(silver_customers_data)\n",
    "    spark.createDataFrame(silver_customers_df).write.mode(\"overwrite\").saveAsTable(f\"{current_catalog}.silver.customer_profiles\")\n",
    "\n",
    "    silver_sessions_data = {\n",
    "        'session_id': [f's{i}' for i in range(20)],\n",
    "        'customer_id': [101 + (i % 10) for i in range(20)],\n",
    "        'session_date': pd.to_datetime(pd.date_range('2025-07-15', periods=20)),\n",
    "        'duration_minutes': [10 + (i % 15) for i in range(20)],\n",
    "        'converted': [i % 3 == 0 for i in range(20)]\n",
    "    }\n",
    "    silver_sessions_df = pd.DataFrame(silver_sessions_data)\n",
    "    spark.createDataFrame(silver_sessions_df).write.mode(\"overwrite\").saveAsTable(f\"{current_catalog}.silver.session_summaries\")\n",
    "\n",
    "    # BRONZE LAYER\n",
    "    bronze_data = {\n",
    "        'user_id': [101 + (i % 10) for i in range(50)],\n",
    "        'event_type': ['page_view', 'click', 'page_view', 'add_to_cart', 'purchase'] * 10,\n",
    "        'timestamp': pd.to_datetime(pd.date_range('2025-07-31 10:00', periods=50, freq='T'))\n",
    "    }\n",
    "    bronze_df = pd.DataFrame(bronze_data)\n",
    "    spark.createDataFrame(bronze_df).write.mode(\"overwrite\").saveAsTable(f\"{current_catalog}.bronze.clickstream_raw\")\n",
    "    \n",
    "    print(\"All tables created and populated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16ae47b3-a7de-4840-b67f-074cbc3686d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Setup Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12a5dfab-f50c-410c-8918-8886e510ccd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Databricks Query Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f45956bc-8fbb-4a89-bab8-0070b3ffe0a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DatabricksQueryTool(BaseTool):\n",
    "    \"\"\"\n",
    "    Tool for executing SQL queries against the Databricks Lakehouse.\n",
    "    \"\"\"\n",
    "    name: str = \"databricks_sql_query\"\n",
    "    description: str = \"\"\"\n",
    "    Executes a SQL query against the Databricks Lakehouse.\n",
    "    Input must be a valid Databricks SQL query.\n",
    "    Example: \"SELECT * FROM my_catalog.silver.customer_profiles WHERE tier = 'premium' LIMIT 10\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Corrected code for DatabricksQueryTool\n",
    "    def _run(\n",
    "        self,\n",
    "        query: str,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Execute a SQL query and return the result as a JSON string.\"\"\"\n",
    "        print(f\"\\nExecuting Databricks Query with original input:\\n---\\n{query}\\n---\\n\")\n",
    "        try:\n",
    "            # More robust cleaning to handle markdown code blocks from the LLM\n",
    "            match = re.search(r\"```(?:sql)?\\s*(.*?)\\s*```\", query, re.DOTALL)\n",
    "            if match:\n",
    "                clean_query = match.group(1).strip()\n",
    "            else:\n",
    "                # THIS IS THE FIX: Only strip whitespace, not quotes.\n",
    "                clean_query = query.strip()\n",
    "            \n",
    "            print(f\"Cleaned Query to be executed: {clean_query}\")\n",
    "\n",
    "            spark = DatabricksSession.builder.getOrCreate()\n",
    "            result_df = spark.sql(clean_query).toPandas()\n",
    "\n",
    "            if result_df.empty:\n",
    "                return \"Query returned no results.\"\n",
    "            \n",
    "            return result_df.to_json(orient='records')\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error executing Databricks query: {str(e)}. Please check your SQL syntax and table/column names.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32884b37-7a99-4031-87a9-bbb8576454ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Schema Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b790632-2c23-48a1-9e2a-6c0e31f27eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SchemaInfoTool(BaseTool):\n",
    "    \"\"\"Tool to get Lakehouse schema information.\"\"\"\n",
    "    name: str = \"get_lakehouse_schema\"\n",
    "    description: str = \"Provides the schema for the key tables in the bronze, silver, and gold layers of the Lakehouse.\"\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        query: str, # The agent might pass an argument, but we ignore it\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Return a pre-defined schema string for the agent to use.\"\"\"\n",
    "        spark = DatabricksSession.builder.getOrCreate()\n",
    "        current_catalog = spark.catalog.currentCatalog()\n",
    "        \n",
    "        return f\"\"\"\n",
    "        DATABRICKS LAKEHOUSE SCHEMA (CURRENT CATALOG: {current_catalog}):\n",
    "\n",
    "        GOLD LAYER (Business-Level Aggregates):\n",
    "        - {current_catalog}.gold.monthly_engagement:\n",
    "            - month (date): The month of the aggregated data.\n",
    "            - avg_engagement_premium (float): Average engagement score for premium customers.\n",
    "            - avg_engagement_standard (float): Average engagement score for standard customers.\n",
    "            - total_revenue (float): Total revenue for the month.\n",
    "            - premium_customer_count (int): Number of premium customers.\n",
    "\n",
    "        SILVER LAYER (Cleaned & Enriched Data):\n",
    "        - {current_catalog}.silver.customer_profiles:\n",
    "            - customer_id (int): Unique identifier for a customer.\n",
    "            - tier (string): Customer tier ('premium' or 'standard').\n",
    "            - signup_date (date): The date the customer signed up.\n",
    "            - lifetime_value (float): The total amount spent by the customer.\n",
    "        - {current_catalog}.silver.session_summaries:\n",
    "            - session_id (string): Unique identifier for a session.\n",
    "            - customer_id (int): The customer who initiated the session.\n",
    "            - session_date (date): The date of the session.\n",
    "            - duration_minutes (int): The duration of the session in minutes.\n",
    "            - converted (boolean): Whether the session resulted in a purchase.\n",
    "\n",
    "        BRONZE LAYER (Raw Data):\n",
    "        - {current_catalog}.bronze.clickstream_raw:\n",
    "            - user_id (int): The ID of the user.\n",
    "            - event_type (string): The type of event (e.g., 'page_view', 'click').\n",
    "            - timestamp (timestamp): The exact time of the event.\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c03477f-d738-4faf-84f6-354069dc5770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Trend Calculator Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a0989cd-8a36-4141-b82e-9a5cc0c616b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json # Make sure to import the json library at the top of your notebook\n",
    "\n",
    "# --- Corrected Trend Calculator Tool ---\n",
    "\n",
    "class TrendCalculatorTool(BaseTool):\n",
    "    \"\"\"Tool to calculate trends and percentage changes.\"\"\"\n",
    "    name: str = \"calculate_trend\"\n",
    "    description: str = \"\"\"\n",
    "    Calculates the percentage change and trend direction between two numeric values.\n",
    "    Input must be a comma-separated string in the format: 'metric_name,current_value,previous_value'\n",
    "    Example: 'Engagement Score,71,83'\n",
    "    \"\"\"\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        query: str,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Calculate the trend and return a descriptive JSON string.\"\"\"\n",
    "        try:\n",
    "            clean_query = query.strip().strip(\"'\\\"\")\n",
    "            \n",
    "            parts = clean_query.split(',')\n",
    "            if len(parts) != 3:\n",
    "                return \"Error: Input format must be 'metric_name,current_value,previous_value'\"\n",
    "\n",
    "            metric_name, current_str, previous_str = parts\n",
    "\n",
    "            current_value = float(current_str.strip().strip(\"'\\\"\"))\n",
    "            previous_value = float(previous_str.strip().strip(\"'\\\"\"))\n",
    "\n",
    "            if previous_value == 0:\n",
    "                change = float('inf') if current_value > 0 else 0.0\n",
    "            else:\n",
    "                change = ((current_value - previous_value) / previous_value) * 100\n",
    "            \n",
    "            direction = \"increased\" if change > 0 else \"decreased\"\n",
    "\n",
    "            # THE FIX: Return a JSON string instead of a sentence.\n",
    "            # This forces the agent to process the data in its next thought step.\n",
    "            result = {\n",
    "                \"metric_name\": metric_name.strip(),\n",
    "                \"trend\": direction,\n",
    "                \"percentage_change\": round(abs(change), 1),\n",
    "                \"from_value\": previous_value,\n",
    "                \"to_value\": current_value\n",
    "            }\n",
    "            return json.dumps(result)\n",
    "\n",
    "        except ValueError as e:\n",
    "            return f\"Error: Invalid number provided. Could not convert to float. Details: {e}\"\n",
    "        except Exception as e:\n",
    "            return f\"An unexpected error occurred during trend calculation: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdfcaccd-f39c-452e-84d8-50437a4e1736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#React Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55796598-c1a8-40ad-9f56-d2173bfa578a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- React Agent Implementation ---\n",
    "\n",
    "class LakehouseReactAgent:\n",
    "    \"\"\"A React Agent for intelligent exploration of a Databricks Lakehouse.\"\"\"\n",
    "\n",
    "    def __init__(self, openai_api_key: str):\n",
    "        \"\"\"Initializes the agent, tools, and LLM.\"\"\"\n",
    "        self.llm = ChatOpenAI(\n",
    "            temperature=0,\n",
    "            model_name=\"gpt-4o\",\n",
    "            openai_api_key=openai_api_key\n",
    "        )\n",
    "\n",
    "        self.tools = [\n",
    "            DatabricksQueryTool(),\n",
    "            SchemaInfoTool(),\n",
    "            TrendCalculatorTool()\n",
    "        ]\n",
    "\n",
    "        # --- NEW: Manually provide the tool names to the prompt ---\n",
    "\n",
    "        # 1. Define the original prompt string with the {tool_names} placeholder.\n",
    "        prompt_with_tool_names = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert data analyst specializing in the Databricks Lakehouse.\n",
    "Your primary goal is to help users understand their business data by forming hypotheses,\n",
    "querying the Lakehouse, and providing clear, actionable insights.\n",
    "\n",
    "**Your Thought Process:**\n",
    "1.  **Understand the Goal:** Fully understand the user's question.\n",
    "2.  **Consult the Schema:** Always start by using the `get_lakehouse_schema` tool to see the available tables and columns. This is mandatory.\n",
    "3.  **Formulate a Plan (Initial):** Start with high-level data (Gold layer) to confirm the primary metric in the user's question.\n",
    "4.  **Execute and Observe:** Use the `databricks_sql_query` tool to execute one query at a time.\n",
    "5.  **Drill-Down and Hypothesize:**\n",
    "    * After confirming the initial finding, **do not stop**. Your main goal is to find the \"why\".\n",
    "    * Formulate a hypothesis about the cause. For example: \"Perhaps the engagement drop is due to lower session durations.\"\n",
    "    * To test your hypothesis, plan a query against a more granular table (e.g., the Silver layer).\n",
    "6.  **Synthesize and Answer:** Once you have gathered enough information from multiple layers, provide a final, comprehensive answer.\n",
    "\n",
    "**Final Answer Structure:**\n",
    "Your final answer MUST be structured with the following markdown sections:\n",
    "- `**Key Finding:**` A one-sentence summary of the main metric change.\n",
    "- `**Business Impact:**` A short paragraph explaining why this change is important.\n",
    "- `**Recommended Next Steps & Hypotheses:**` A numbered list of concrete actions or queries to investigate further. This is the most important part of your answer.\n",
    "\n",
    "**Critical Rules:**\n",
    "- **SQL Syntax:** Use Databricks SQL. Use single quotes (') for strings. Use the full three-level namespace.\n",
    "- **THE MOST RELIABLE WAY TO FILTER DATES:** To compare a date/timestamp column with a string, always CAST the string to a DATE. Example: `WHERE month = CAST('2025-07-31' AS DATE)`.\n",
    "- **Tool Input:** Ensure your 'Action Input' is a clean, raw string for the tool.\n",
    "\n",
    "**Tools Available:**\n",
    "{tools}\n",
    "\n",
    "**Interaction Format:**\n",
    "\n",
    "Question: The user's question you need to answer.\n",
    "Thought: Your reasoning and plan for the next action.\n",
    "Action: The tool to use, chosen from [{tool_names}].\n",
    "Action Input: The exact input for the chosen tool.\n",
    "Observation: The result returned by the tool.\n",
    "... (this Thought/Action/Action Input/Observation cycle can repeat multiple times)\n",
    "Thought: I have now gathered all the necessary information to answer the user's question.\n",
    "Final Answer: Your final, well-structured answer to the original question.\n",
    "\n",
    "---\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\n",
    "\"\"\")\n",
    "\n",
    "        # 2. Get the tool names from your tools list.\n",
    "        tool_names = \", \".join([tool.name for tool in self.tools])\n",
    "\n",
    "        # 3. Use the .partial() method to \"pre-fill\" the tool_names variable.\n",
    "        self.prompt = prompt_with_tool_names.partial(tool_names=tool_names)\n",
    "        self.agent = create_react_agent(\n",
    "            llm=self.llm,\n",
    "            tools=self.tools,\n",
    "            prompt=self.prompt\n",
    "        )\n",
    "\n",
    "        self.agent_executor = AgentExecutor(\n",
    "            agent=self.agent,\n",
    "            tools=self.tools,\n",
    "            verbose=True,\n",
    "            max_iterations=12,\n",
    "            handle_parsing_errors=\"Check your input format and try again.\",\n",
    "            return_intermediate_steps=True\n",
    "        )\n",
    "\n",
    "    def analyze(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyzes a business question using the React Agent.\"\"\"\n",
    "        try:\n",
    "            result = self.agent_executor.invoke({\"input\": question})\n",
    "            return {\n",
    "                \"answer\": result.get(\"output\", \"No output generated.\"),\n",
    "                \"intermediate_steps\": result.get(\"intermediate_steps\", []),\n",
    "                \"success\": True\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": f\"An error occurred during agent execution: {str(e)}\",\n",
    "                \"intermediate_steps\": [],\n",
    "                \"success\": False\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b8fa299-cc1a-4c4d-a066-d3f1028e502a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Demo Agent (Delete Later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7301f8d7-d27c-4e8d-919e-8be0f1ff0fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Demo Agent for Educational Purposes ---\n",
    "\n",
    "class DemoReactAgent:\n",
    "    \"\"\"\n",
    "    A simulated React Agent that demonstrates the reasoning process\n",
    "    without requiring API keys or a live Databricks connection.\n",
    "    Perfect for book examples and offline demonstrations.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.tools = {\n",
    "            'get_lakehouse_schema': SchemaInfoTool(),\n",
    "            'calculate_trend': TrendCalculatorTool()\n",
    "        }\n",
    "\n",
    "    def analyze(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Simulates the step-by-step reasoning of the React Agent.\"\"\"\n",
    "        print(f\"Analyzing Question: {question}\\n\")\n",
    "        print(\"--- DEMO AGENT REASONING LOG ---\")\n",
    "\n",
    "        steps = []\n",
    "        \n",
    "        thought1 = \"First, I need to understand the available data. I will use the get_lakehouse_schema tool.\"\n",
    "        print(f\"Thought: {thought1}\")\n",
    "        action1 = \"get_lakehouse_schema\"\n",
    "        print(f\"Action: {action1}\")\n",
    "        action_input1 = \"\"\n",
    "        print(f\"Action Input: {action_input1}\")\n",
    "        observation1 = self.tools['get_lakehouse_schema']._run(action_input1)\n",
    "        print(f\"Observation: {observation1}\")\n",
    "        steps.append((thought1, action1, action_input1, observation1))\n",
    "\n",
    "        thought2 = \"The user is asking about premium customer engagement. The `gold.monthly_engagement` table looks like the best place to start. I'll get the last two months of data to see the trend.\"\n",
    "        print(f\"Thought: {thought2}\")\n",
    "        action2 = \"databricks_sql_query\"\n",
    "        print(f\"Action: {action2}\")\n",
    "        action_input2 = \"SELECT month, avg_engagement_premium FROM my_catalog.gold.monthly_engagement ORDER BY month DESC LIMIT 2\"\n",
    "        print(f\"Action Input: {action_input2}\")\n",
    "        observation2 = '[{\"month\": \"2025-07-31\", \"avg_engagement_premium\": 71.0}, {\"month\": \"2025-06-30\", \"avg_engagement_premium\": 83.0}]'\n",
    "        print(f\"Observation: {observation2}\")\n",
    "        steps.append((thought2, action2, action_input2, observation2))\n",
    "\n",
    "        thought3 = \"The observation shows a drop in engagement from 83.0 to 71.0. I need to calculate the exact percentage change to quantify this drop.\"\n",
    "        print(f\"Thought: {thought3}\")\n",
    "        action3 = \"calculate_trend\"\n",
    "        print(f\"Action: {action3}\")\n",
    "        action_input3 = \"Premium Engagement,71.0,83.0\"\n",
    "        print(f\"Action Input: {action_input3}\")\n",
    "        observation3 = self.tools['calculate_trend']._run(action_input3)\n",
    "        print(f\"Observation: {observation3}\")\n",
    "        steps.append((thought3, action3, action_input3, observation3))\n",
    "        \n",
    "        thought4 = \"I have confirmed the drop and quantified it. I now have enough information to provide a final answer.\"\n",
    "        print(f\"Thought: {thought4}\")\n",
    "        final_answer = \"\"\"\n",
    "**Analysis of Premium Customer Engagement Drop**\n",
    "\n",
    "**Key Finding:**\n",
    "There was a significant **14.5% decrease** in the average engagement score for premium customers, falling from 83.0 in June 2025 to 71.0 in July 2025.\n",
    "\n",
    "**Business Impact:**\n",
    "A drop of this magnitude is a strong indicator of declining customer satisfaction or perceived value among your most important customer segment. This could be a leading indicator of future churn and revenue loss if not addressed promptly.\n",
    "\n",
    "**Recommended Next Steps & Hypotheses:**\n",
    "1.  **Drill-Down Analysis:** Investigate if this drop is correlated with a decrease in session duration or conversion rates by querying the `silver.session_summaries` table.\n",
    "2.  **Product Changes:** Was a key feature deprecated or changed in late June or early July?\n",
    "3.  **Marketing Campaigns:** Did a competitor launch a new, attractive offer that could be pulling customers away?\n",
    "        \"\"\"\n",
    "        print(f\"Final Answer: {final_answer}\")\n",
    "\n",
    "        return {\n",
    "            \"answer\": final_answer,\n",
    "            \"intermediate_steps\": steps,\n",
    "            \"success\": True\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "875b3da4-90bf-4ae0-aff3-4e73a38df869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#ReAct Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "919679ae-0c01-4b56-ac51-d81f9b2bcf0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "\n",
    "\"\"\"Main function to run the agent.\"\"\"\n",
    "print(\"=\"*80)\n",
    "print(\"ü§ñ Welcome to the React Agent for Databricks Lakehouse Intelligence ü§ñ\")\n",
    "print(\"=\"*80)\n",
    "print(\"This script demonstrates how an AI agent can intelligently query and analyze data.\")\n",
    "print(\"\\nChoose an execution mode:\")\n",
    "print(\"1. Demo Mode (Recommended for first-time use, no API key or Databricks setup needed)\")\n",
    "print(\"2. Live Agent Mode (Requires OpenAI API key and Databricks Connect configuration)\")\n",
    "\n",
    "print(\"\\n--- Live Agent Mode ---\")\n",
    "try:\n",
    "    openai_api_key = dbutils.secrets.get(scope=\"book\", key=\"OPENAI_API_KEY\")\n",
    "    print(f\"OpenAI API key retrieved successfully.\")                                \n",
    "except Exception as e:\n",
    "    print(\"‚ùå ERROR: Could not retrieve OpenAI API key from Databricks Secrets.\")\n",
    "    print(\"Please ensure the secret scope 'my-react-agent' and key 'openai-api-key' are configured correctly.\")\n",
    "    exit(0)\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"‚ùå ERROR: OPENAI_API_KEY environment variable not set.\")\n",
    "    print(\"Please set it to run the live agent. Exiting.\")\n",
    "    exit(0)\n",
    "\n",
    "try:\n",
    "    print(\"Verifying Databricks Connect configuration...\")\n",
    "    spark = DatabricksSession.builder.getOrCreate()\n",
    "    print(\"‚úÖ Databricks session created successfully.\")\n",
    "    \n",
    "    # Set up the mock data environment for the live session\n",
    "    setup_lakehouse_environment()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Databricks Connect is not configured correctly. {e}\")\n",
    "    print(\"Please run 'databricks configure' in your terminal. Exiting.\")\n",
    "    exit(0)\n",
    "\n",
    "agent = LakehouseReactAgent(openai_api_key=openai_api_key)\n",
    "question = \"Why did our premium customers' engagement drop last month?\"\n",
    "print(f\"\\nüöÄ Asking the live agent: '{question}'\")\n",
    "result = agent.analyze(question)\n",
    "print(\"\\n--- FINAL AGENT RESPONSE ---\")\n",
    "print(result[\"answer\"])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "10-01-React Agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
